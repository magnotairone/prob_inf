---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Probabilidade Condicional

Em ciência de dados, raramente olhamos para a população inteira ($\Omega$). Nosso trabalho é quase sempre "fatiar" os dados para entender subpopulações.

* "Qual a taxa de conversão *apenas* dos usuários que vieram do Facebook?"
* "Qual a probabilidade de *churn* *dado que* o cliente é do segmento VIP?"
* "Qual a chance de fraude *neste* usuário, *dado que* sua transação foi 10x maior que a média?"

Em todos esses casos, não estamos perguntando $\mathbb{P}(A)$, mas sim $\mathbb{P}(A|B)$. Estamos pedindo a probabilidade de $A$ (conversão, *churn*, fraude) depois de **reduzirmos nosso universo** $\Omega$ para um subconjunto $B$ (usuários do Facebook, clientes VIP, transações atípicas). A probabilidade condicional é a formalização matemática dessa "redução de universo" ou "filtro". 

::: {#def-prob-condicional name="Definição 1.5 (Probabilidade Condicional)"}
Seja $(\Omega, \mathcal{F}, \mathbb{P})$ um espaço de probabilidade. Se $B \in \mathcal{F}$, com $\mathbb{P}(B) > 0,$ a **probabilidade condicional** de A dado B é definida por
$$ \mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}, \quad A \in \mathcal{F} $$
:::

**Notas:**

1)  Se $\mathbb{P}(B)=0$, definimos $\mathbb{P}(A|B) = \mathbb{P}(A)$.
2)  $\mathbb{P}(\cdot|B)$ é uma medida de probabilidade em $(\Omega, \mathcal{F})$ (@exm-prob-condicional).

> **Perspectiva de Data Science: $\mathbb{P}(A|B)$ como uma Métrica Filtrada** 
>
> A fórmula $\mathbb{P}(A|B) = \mathbb{P}(A \cap B) / \mathbb{P}(B)$ é a justificativa para a maioria das nossas *queries* de análise.
>
> * $\mathbb{P}(A \cap B)$ = `COUNT(usuários que são VIP E converteram)` / `COUNT(total de usuários)`
> * $\mathbb{P}(B)$ = `COUNT(usuários que são VIP)` / `COUNT(total de usuários)`
>
> Quando dividimos um pelo outro, o `COUNT(total de usuários)` (o denominador $\Omega$) cancela:
>
> $\mathbb{P}(A|B) = \frac{\text{COUNT(VIP e Converteu)}}{\text{COUNT(VIP)}}$
>
> Esta é exatamente a métrica de "taxa de conversão do segmento VIP". A teoria da probabilidade condicional é o que garante que essa nova métrica, calculada sobre o *dataset* filtrado, ainda obedece a todas as regras da probabilidade.

::: {#thm-regra-produto name="Teorema 1.2 (Regra do Produto)"}
Seja $(\Omega, \mathcal{F}, \mathbb{P})$ um espaço de probabilidade. Então
$$ \mathbb{P}(A_1 \cap A_2 \cap \dots \cap A_n) = \mathbb{P}(A_1) \mathbb{P}(A_2|A_1) \mathbb{P}(A_3|A_1 \cap A_2) \dots \mathbb{P}(A_n|\bigcap_{k=1}^{n-1} A_k) $$
para todo $A_1, A_2, \dots, A_n \in \mathcal{F}$, $n=2, 3, \dots$

**Demonstração:** exercício lista 2.
:::

::: {#exm-funil-conversao name="Exemplo 1.12 (Funil de Conversão)"}
Um site de e-commerce analisa seu funil de conversão. Um usuário passa por 3 etapas: 1) Visita a página, 2) Adiciona um item ao carrinho, 3) Conclui a compra.

Sejam os eventos:

* $A_1$ = "Usuário visita a página"
* $A_2$ = "Usuário adiciona ao carrinho"
* $A_3$ = "Usuário conclui a compra"

Queremos a probabilidade de um usuário passar pelo funil completo: $\mathbb{P}(A_1 \cap A_2 \cap A_3)$.

O time de produto nos informa as seguintes métricas (probabilidades condicionais):

* $\mathbb{P}(A_1) = 1.0$ (Assumimos que estamos analisando apenas usuários que visitaram)
* $\mathbb{P}(A_2 | A_1) = 0.10$ (10% dos visitantes adicionam ao carrinho)
* $\mathbb{P}(A_3 | A_1 \cap A_2) = 0.40$ (40% dos que visitam E adicionam ao carrinho, concluem a compra)

Pela Regra do Produto:
$$ \mathbb{P}(A_1 \cap A_2 \cap A_3) = \mathbb{P}(A_1) \mathbb{P}(A_2|A_1) \mathbb{P}(A_3|A_1 \cap A_2) $$
$$ = 1.0 \times 0.10 \times 0.40 = 0.04 $$
A taxa de conversão de ponta-a-ponta do funil é de 4%.
:::


Dizemos que $A_1, A_2, \dots \in \mathcal{F}$ formam uma **partição** de $\Omega$ se são (mutuamente) disjuntos e $\bigcup_{k=1}^{\infty} A_k = \Omega$.

Dessa forma, para todo evento $B \subseteq \Omega$,
$$ B = \bigcup_{k=1}^{\infty} (A_k \cap B) $$

::: {#thm-lei-prob-total name="Teorema 1.3 (Lei da Probabilidade Total)"}
Se $A_1, A_2, \dots$ (sequência finita ou enumerável) de eventos formar uma partição de $\Omega$, então
$$ \mathbb{P}(B) = \sum_{k=1}^{\infty} \mathbb{P}(A_k) \mathbb{P}(B|A_k), \quad \forall B \in \mathcal{F} $$
:::

> **Perspectiva de Data Science: A Lei da Média Ponderada**
>
> A Lei da Probabilidade Total é como "reconstruímos" uma métrica geral a partir de seus segmentos.
>
> Se $B$ é o evento "converter", e a partição $A_k$ são os segmentos de clientes ("VIP", "Regular", "Novo"), então:
>
> $\mathbb{P}(\text{Converter}) = \sum_{k} \mathbb{P}(\text{Converter} | \text{Segmento}_k) \times \mathbb{P}(\text{Segmento}_k)$
>
> A taxa de conversão *geral* é a **média ponderada** das taxas de conversão de cada segmento, onde os pesos são o tamanho (probabilidade) de cada segmento. Isso é usado o tempo todo para entender *drivers* de uma métrica: a métrica geral subiu porque um segmento melhorou (o $\mathbb{P}(B|A_k)$) ou porque um segmento de alta performance cresceu (o $\mathbb{P}(A_k)$)?

Usando o @thm-lei-prob-total, podemos obter a **fórmula de Bayes**:
$$ \mathbb{P}(A_i|B) = \frac{\mathbb{P}(A_i \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(A_i) \mathbb{P}(B|A_i)}{\sum_{k=1}^{\infty} \mathbb{P}(A_k) \mathbb{P}(B|A_k)} $$

> **Perspectiva de Data Science: Teorema de Bayes, O Motor da Inferência**
>
> O Teorema de Bayes é talvez o mais importante da estatística para a ciência de dados. Ele nos permite "inverter a pergunta":
>
> * Nós *sabemos* $\mathbb{P}(B|A_i)$: A probabilidade de *ver uma evidência* (B), dado que uma *causa* ($A_i$) é verdadeira.
>     * Ex: $\mathbb{P}(\text{clique} | \text{é fraude})$ (Fácil de medir a partir de dados de fraude confirmada).
> * Nós *queremos saber* $\mathbb{P}(A_i|B)$: A probabilidade da *causa* ser verdadeira, dado que vimos a *evidência*.
>     * Ex: $\mathbb{P}(\text{é fraude} | \text{clique})$ (A pergunta crucial: esse clique *foi* fraudulento?)
>
> O Teorema de Bayes é a base para:
>
> * **Filtros de Spam:** $\mathbb{P}(\text{Spam} | \text{Palavra "Viagra"})$
> * **Detecção de Anomalias:** $\mathbb{P}(\text{Fraude} | \text{Transação > 10k})$
> * **Classificadores Naive Bayes:** Um modelo de *machine learning* inteiro que usa Bayes para classificar textos, diagnosticar doenças, etc.
>
> Onde $\mathbb{P}(A_i)$ é o **Prior** (nossa crença inicial, ex: a taxa base de fraude) e $\mathbb{P}(A_i|B)$ é o **Posterior** (nossa crença atualizada após ver a evidência $B$).


## Independência de Eventos

::: {#def-independencia name="Definição 1.6 (Independência)"}
Seja $(\Omega, \mathcal{F}, \mathbb{P})$ um espaço de probabilidade. Os eventos A e B são **independentes** se, e somente se,
$$ \mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B) $$
:::

**Notas:**

1)  Eventos de probabilidade zero ou um são independentes de qualquer outro.
2)  Dois eventos são independentes se, e somente se, $\mathbb{P}(A|B) = \mathbb{P}(A)$.
3)  Se $A \cap B = \emptyset,$ então A e B não são independentes (a menos que um deles tenha probabilidade zero).

::: {#prp-indep-si-mesmo name="Proposição 1.3"}
A é independente de si mesmo se, e somente se, $\mathbb{P}(A)=0$ ou $\mathbb{P}(A)=1$.

**Demonstração:**
De fato,
$\mathbb{P}(A \cap A) = \mathbb{P}(A) \mathbb{P}(A) \Leftrightarrow \mathbb{P}(A) = [\mathbb{P}(A)]^2$
$\Leftrightarrow \mathbb{P}(A) = 0 \text{ ou } \mathbb{P}(A) = 1$.
:::

::: {#prp-indep-complemento name="Proposição 1.4"}
Se A e B são independentes, então A e $B^c$ também são independentes. (e também $A^c$ e B, e ainda $A^c$ e $B^c$).

**Demonstração:**
$A \text{ e } B \text{ indep.} \Rightarrow \mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)$
Temos $A = (A \cap B) \cup (A \cap B^c)$.
$$ \mathbb{P}(A \cap B^c) = \mathbb{P}(A - (A \cap B)) = \mathbb{P}(A) - \mathbb{P}(A \cap B) $$
$$ = \mathbb{P}(A) - \mathbb{P}(A)\mathbb{P}(B) $$
$$ = \mathbb{P}(A) [1 - \mathbb{P}(B)] = \mathbb{P}(A) \mathbb{P}(B^c). $$
:::

::: {#def-indep-2a2 name="Definição 1.7"}
Os eventos aleatórios $\{A_i\}_{i \in I}$ são **independentes 2 a 2** se
$$ \mathbb{P}(A_i \cap A_j) = \mathbb{P}(A_i) \mathbb{P}(A_j), \quad \forall i, j \in I, i \neq j $$
:::

::: {#def-indep-coletiva name="Definição 1.8"}
Os eventos aleatórios $\{A_i\}_{i \in I}$ são **(mutuamente) independentes** se, e só se, dado qualquer conjunto de índices distintos $i_1, \dots, i_n \in I$, $n \ge 2$, vale
$$ \mathbb{P}(A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_n}) = \prod_{k=1}^{n} \mathbb{P}(A_{i_k}) $$
:::

::: {#exm-2a2-vs-coletiva name="Exemplo 1.14"}
Seja $\Omega = \{u_1, u_2, u_3, u_4\}$ (4 usuários), com $\mathbb{P}(\{u_i\}) = 1/4, \forall i$.
Defina os eventos (características dos usuários):

* $A$ = "Engajamento Alto" = $\{u_2, u_4\}$
* $B$ = "Região Sudeste" = $\{u_1, u_2\}$
* $C$ = "Acesso via Mobile" = $\{u_1, u_4\}$

Verifique que A, B e C são 2 a 2 independentes, mas não coletivamente independentes.


$\mathbb{P}(A) = 2/4 = 1/2$
$\mathbb{P}(B) = 2/4 = 1/2$
$\mathbb{P}(C) = 2/4 = 1/2$

$\mathbb{P}(A \cap B) = \mathbb{P}(\{u_2\}) = 1/4$. Logo $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$. (A e B são indep.)

$\mathbb{P}(B \cap C) = \mathbb{P}(\{u_1\}) = 1/4$. Logo $\mathbb{P}(B \cap C) = \mathbb{P}(B)\mathbb{P}(C)$. (B e C são indep.)

$\mathbb{P}(A \cap C) = \mathbb{P}(\{u_4\}) = 1/4$. Logo $\mathbb{P}(A \cap C) = \mathbb{P}(A)\mathbb{P}(C)$. (A e C são indep.)

**Mas:**
$\mathbb{P}(A \cap B \cap C) = \mathbb{P}(\emptyset) = 0$.

$\mathbb{P}(A)\mathbb{P}(B)\mathbb{P}(C) = (1/2)^3 = 1/8$.

Como $0 \neq 1/8$, os eventos não são mutuamente independentes. Saber B e C juntos nos diz que o usuário é $u_1$, o que *exclui* a possibilidade de ser A.
:::

::: {#exm-uniao-indep name="Exemplo 1.15"}
Mostre que se $A_1, A_2, \dots, A_n$ são independentes e $\bigcup_{i=1}^n A_i = \Omega$, então $\exists i$ tal que $\mathbb{P}(A_i) = 1$.

**Demonstração:**
$\bigcup_{i=1}^n A_i = \Omega \Rightarrow \mathbb{P}(\bigcup_{i=1}^n A_i) = 1$.
Pela Lei de De Morgan:
$1 = \mathbb{P}((\bigcap_{i=1}^n A_i^c)^c) = 1 - \mathbb{P}(\bigcap_{i=1}^n A_i^c)$

$\Rightarrow \mathbb{P}(\bigcap_{i=1}^n A_i^c) = 0$.

$A_i$ independentes $\Rightarrow A_i^c$ independentes $\Rightarrow \prod_{i=1}^n \mathbb{P}(A_i^c) = 0$.

Então, $\exists i$ tal que $\mathbb{P}(A_i^c) = 0 \Rightarrow \mathbb{P}(A_i) = 1$.
:::

**Exercício 19 (lista 2)**
$(\Omega, \mathcal{F}, \mathbb{P})$ espaço de probabilidade, $A \in \mathcal{F}$, com $\mathbb{P}(A) \in (0, 1)$.
A e B são independentes se, e só se, $\mathbb{P}(B|A) = \mathbb{P}(B|A^c)$.

<!-- TODO: verificar -->
> **Perspectiva de Data Science: A Hipótese Nula de um Teste A/B**
>
> Este exercício é a **definição matemática da hipótese nula ($H_0$)** em um teste A/B.
>
> * $B$ = O evento de interesse (ex: "Compra")
> * $A$ = "Usuário viu a Versão A (Tratamento)"
> * $A^c$ = "Usuário viu a Versão B (Controle)"
>
> A afirmação $\mathbb{P}(B|A) = \mathbb{P}(B|A^c)$ se traduz em:
> "A taxa de compra no grupo Tratamento é *exatamente igual* à taxa de compra no grupo Controle."
>
> O exercício prova que isso é matematicamente equivalente a dizer que "fazer a compra" ($B$) é *independente* do "grupo que o usuário viu" ($A$).
>
> Nosso objetivo no teste é justamente tentar **rejeitar** essa afirmação, provando que os eventos são *dependentes* e que $\mathbb{P}(B|A) \neq \mathbb{P}(B|A^c)$.

## Implementação Prática em R

Vamos usar o *dataset* da aula anterior para verificar esses conceitos na prática.

```{r}
# Criar nosso universo (Omega)
set.seed(42)
Omega_data <- data.frame(
  user_id = 1:1000,
  segmento = sample(c("VIP", "Regular", "Novo"), 1000, replace = TRUE, prob = c(0.1, 0.6, 0.3)),
  clicou = sample(c(0, 1), 1000, replace = TRUE, prob = c(0.8, 0.2))
)

# Definindo nossa função de probabilidade empírica
P <- function(evento_subset) {
  nrow(evento_subset) / nrow(Omega_data)
}
```

Qual a probabilidade de um usuário "clicar" ($A$) *dado que* ele é "VIP" ($B$)?

```{r}
#| warning: false
#| message: false

# P(A|B) = P(A ∩ B) / P(B)

library(tidyverse)

# Eventos
A_clicou <- Omega_data |> filter(clicou == 1)
B_vip <- Omega_data |> filter(segmento == "VIP")
A_e_B <- Omega_data |> filter(clicou == 1 & segmento == "VIP")

# Probabilidades
P_A_e_B <- P(A_e_B)
P_B <- P(B_vip)

P_A_dado_B <- P_A_e_B / P_B
cat("P(Clicou | VIP) via fórmula:", P_A_dado_B, "\n")
```

```{r}
# Forma Direta (Reduzindo o Universo)
# 1. Reduzimos nosso universo para Omega' = B_vip
Omega_filtrado <- Omega_data |> filter(segmento == "VIP")

# 2. Calculamos a probabilidade de A dentro desse NOVO universo
# Note que aqui o denominador é nrow(Omega_filtrado)
P_A_no_universo_B <- sum(Omega_filtrado$clicou == 1) / nrow(Omega_filtrado)

cat("P(Clicou | VIP) via filtro direto:", P_A_no_universo_B, "\n")

```

Os resultados são idênticos, validando a definição.

Os eventos $A$="clicou" e $B$="VIP" são independentes?
Verificamos se $\mathbb{P}(A \cap B) = \mathbb{P}(A) \times \mathbb{P}(B)$.

```{r}
P_A <- P(A_clicou)
# P_B e P_A_e_B já foram calculados

cat("Verificando Independência (Clicou e VIP):\n")
cat("  Lado Esquerdo: P(A ∩ B) =", P_A_e_B, "\n")
cat("  Lado Direito: P(A) * P(B) =", P_A * P_B, "\n")

```
Como os valores são diferentes, os eventos NÃO são independentes.
Saber que um usuário é "VIP" MUDA a probabilidade de ele "clicar".


A partição do nosso universo são os segmentos: $A_1$="VIP", $A_2$="Regular", $A_3$="Novo".
O evento $B$ é "clicou".

Vamos "remontar" a $\mathbb{P}(B)$ geral usando a fórmula:
$\mathbb{P}(B) = \mathbb{P}(B|A_1)\mathbb{P}(A_1) + \mathbb{P}(B|A_2)\mathbb{P}(A_2) + \mathbb{P}(B|A_3)\mathbb{P}(A_3)$

```{r}
# 1. Probabilidades da Partição P(A_k)
P_A1_vip <- P(Omega_data |> filter(segmento == "VIP"))
P_A2_reg <- P(Omega_data |> filter(segmento == "Regular"))
P_A3_nov <- P(Omega_data |> filter(segmento == "Novo"))

# 2. Probabilidades Condicionais P(B | A_k)
# Usando o método direto (filtrado)
P_B_dado_A1 <- Omega_data |> 
  filter(segmento == "VIP") |> 
  summarise(media = mean(clicou)) |> 
  pull(media)

P_B_dado_A2 <- Omega_data |> 
  filter(segmento == "Regular") |> 
  summarise(media = mean(clicou)) |> 
  pull(media)

P_B_dado_A3 <- Omega_data |> 
  filter(segmento == "Novo") |> 
  summarise(media = mean(clicou)) |> 
  pull(media)

cat("P(Clicou|VIP):", P_B_dado_A1, "\n")
cat("P(Clicou|Regular):", P_B_dado_A2, "\n")
cat("P(Clicou|Novo):", P_B_dado_A3, "\n")

# 3. Média Ponderada
P_B_calculada <- (P_B_dado_A1 * P_A1_vip) + 
                 (P_B_dado_A2 * P_A2_reg) + 
                 (P_B_dado_A3 * P_A3_nov)

cat("P(B) via Lei Total (Média Ponderada):", P_B_calculada, "\n")

# 4. Probabilidade Real (Geral)
P_B_real <- P(Omega_data |> filter(clicou == 1))
cat("P(B) Real (Taxa Geral de Clique):", P_B_real, "\n")
```

Os valores são idênticos, validando a Lei da Probabilidade Total.