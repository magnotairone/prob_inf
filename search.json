[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilidade e Inferência Estatística para Ciência de Dados",
    "section": "",
    "text": "Introdução\nNotas de aula de um curso de Probabilidade e Inferência para Ciência de Dados. Baseado nas notas de aula do curso MAE5702 do professor Felipe de Queiroz, a quem agradeço pela gentil disponibilização do material.",
    "crumbs": [
      "Introdução"
    ]
  },
  {
    "objectID": "010_probabilidade.html",
    "href": "010_probabilidade.html",
    "title": "Módulo Probabilidade",
    "section": "",
    "text": "TODO: descrever essa parte",
    "crumbs": [
      "Módulo Probabilidade"
    ]
  },
  {
    "objectID": "aula01.html",
    "href": "aula01.html",
    "title": "1  Introdução: Da Teoria à Prática Analítica",
    "section": "",
    "text": "1.1 A Linguagem dos Dados: Conjuntos e Eventos\nImagine que você trabalha como Cientista de Dados em uma empresa de streaming de música. Uma pergunta fundamental do negócio é: “Quais são os nossos perfis de usuários? Quem são os ouvintes leais e quem são os esporádicos?”. Para responder a isso, você tem acesso aos dados de login de cada usuário, mês a mês.\nComo poderíamos definir matematicamente o que significa ser um “ouvinte leal”? Seria alguém que logou todos os meses? Ou alguém que, a partir de certo ponto, nunca mais deixou de logar? E o “ouvinte esporádico”? Seria aquele que, mesmo que desapareça por alguns meses, sempre acaba voltando?\nPara responder a essas perguntas de forma precisa e rigorosa, precisamos de uma linguagem formal. Essa linguagem é a Teoria dos Conjuntos. Nesta aula, vamos construir o alicerce matemático que nos permitirá não apenas estruturar nosso pensamento analítico, mas também desenvolver as ferramentas para analisar o comportamento de sistemas que evoluem ao longo do tempo. Cada definição e proposição que veremos é um passo em direção à solução do nosso problema.\nPara analisar dados, primeiro precisamos defini-los. A teoria dos conjuntos nos fornece o vocabulário fundamental para isso.",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução: Da Teoria à Prática Analítica</span>"
    ]
  },
  {
    "objectID": "aula01.html#a-linguagem-dos-dados-conjuntos-e-eventos",
    "href": "aula01.html#a-linguagem-dos-dados-conjuntos-e-eventos",
    "title": "1  Introdução: Da Teoria à Prática Analítica",
    "section": "",
    "text": "Definição 1.1 (Conjunto) Um conjunto \\(\\Omega\\) é uma coleção de objetos distintos, que serão denotados por \\(\\omega\\).\n\n\\(\\omega \\in \\Omega\\) (elemento \\(\\omega\\) pertence ao conjunto \\(\\Omega\\)).\n\\(\\omega \\notin \\Omega\\) (elemento \\(\\omega\\) não pertence ao conjunto \\(\\Omega\\)).\n\n\n\nPerspectiva de Data Science: Pense no conjunto universal \\(\\Omega\\) como todo o seu universo de dados, ou espaço amostral. Cada elemento \\(\\omega\\) é uma unidade observacional: um cliente, uma transação, um produto. Por exemplo, \\(\\Omega\\) pode ser “o conjunto de todos os usuários da nossa plataforma”.\n\n\nDefinição 1.2 (Subconjunto) Dizemos que A é um subconjunto de \\(\\Omega\\), ou que A está contido em \\(\\Omega\\), e denotamos por \\(A \\subseteq \\Omega\\), se \\(\\forall \\omega \\in A \\rightarrow \\omega \\in \\Omega\\).\n\n\nPerspectiva de Data Science: Um subconjunto é um segmento de interesse ou um evento dentro do seu universo de dados, geralmente obtido através de um filtro ou consulta.\n\nSe \\(\\Omega\\) é o conjunto de todos os usuários, um subconjunto \\(A\\) pode ser: \\(A = \\{\\)usuários do plano Premium\\(\\}\\).\nOutro subconjunto \\(B\\) poderia ser: \\(B = \\{\\)usuários que ouviram mais de 100 horas de música no último mês\\(\\}\\).",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução: Da Teoria à Prática Analítica</span>"
    ]
  },
  {
    "objectID": "aula01.html#a-gramática-da-análise-operações-com-conjuntos",
    "href": "aula01.html#a-gramática-da-análise-operações-com-conjuntos",
    "title": "1  Introdução: Da Teoria à Prática Analítica",
    "section": "1.2 A Gramática da Análise: Operações com Conjuntos",
    "text": "1.2 A Gramática da Análise: Operações com Conjuntos\nCom nossos segmentos definidos, precisamos de uma forma de combiná-los e compará-los. As operações com conjuntos são a “gramática” que nos permite realizar análises complexas.\nSejam A, \\(A_1\\), \\(A_2\\),… subconjuntos de \\(\\Omega\\). Temos as seguintes operações:\n\nComplementar de A: \\(A^{c} = \\{ \\omega \\in \\Omega : \\omega \\notin A \\}\\).\nUnião: \\(\\bigcup_{i=1}^{n} A_{i} = \\{ \\omega \\in \\Omega : \\omega \\in A_{i} \\text{ para ao menos um } i=1,2,...,n \\}\\).\nIntersecção: \\(\\bigcap_{i=1}^{n} A_{i} = \\{ \\omega \\in \\Omega : \\omega \\in A_{i}, \\forall i=1,...,n \\}\\).\nDiferença: \\(A_{1} - A_{2} = \\{ \\omega \\in \\Omega : \\omega \\in A_{1}, \\omega \\notin A_{2} \\} = A_{1} \\cap A_{2}^{c}\\).\nDiferença simétrica: \\(A_{1} \\Delta A_{2} = (A_{1} - A_{2}) \\cup (A_{2} - A_{1}) = (A_{1} \\cap A_{2}^{c}) \\cup (A_{1}^{c} \\cap A_{2})\\).\n\n\nConjunto vazio (\\(\\emptyset\\)): não contém nenhum elemento.\n\n\nPerspectiva de Data Science: Cada operação corresponde diretamente a uma operação lógica em uma consulta de dados:\n\nIntersecção (\\(A \\cap B\\)) é a lógica E (AND). Ex: “Usuários do plano Premium E que ouviram mais de 100 horas”.\nUnião (\\(A \\cup B\\)) é a lógica OU (OR). Ex: “Usuários do plano Premium OU que ouviram mais de 100 horas”.\nComplementar (\\(A^c\\)) é a lógica NÃO (NOT). Ex: “Usuários que NÃO são do plano Premium”.\n\n\n\nDefinição 1.3 (Relações entre Conjuntos)  \n\nDizemos que \\(A_{1}\\) e \\(A_{2}\\) são disjuntos se \\(A_{1} \\cap A_{2} = \\emptyset\\).\nDizemos que \\(A_{1} = A_{2}\\) se \\(A_{1} \\subseteq A_{2}\\) e \\(A_{2} \\subseteq A_{1}\\).\nDizemos que \\(A_1, A_2, ...\\) são mutuamente disjuntos se \\(A_{i} \\cap A_{j} = \\emptyset\\), \\(\\forall i \\neq j\\).\n\n\n\nProposição 1.1 (Lei de De Morgan) Sejam \\(A_{1}, A_{2}, ...\\) subconjuntos de \\(\\Omega\\). Então:\n\n\\((\\bigcup_{i=1}^{\\infty} A_{i})^{c} = \\bigcap_{i=1}^{\\infty} A_{i}^{c}\\)\n\\((\\bigcap_{i=1}^{\\infty} A_{i})^{c} = \\bigcup_{i=1}^{\\infty} A_{i}^{c}\\)\n\n\n\nNota: As Leis de De Morgan são extremamente úteis para simplificar consultas lógicas complexas. A negação de uma condição “OU” ampla é o mesmo que exigir que todas as condições “E” individuais sejam falsas.\n\nDemonstração (a):\nPrecisamos mostrar que \\((\\bigcup_{i=1}^{\\infty} A_{i})^{c} \\subseteq \\bigcap_{i=1}^{\\infty} A_{i}^{c}\\) e \\(\\bigcap_{i=1}^{\\infty} A_{i}^{c} \\subseteq (\\bigcup_{i=1}^{\\infty} A_{i})^{c}.\\)\nParte 1: \\((\\bigcup_{i=1}^{\\infty} A_{i})^{c} \\subseteq \\bigcap_{i=1}^{\\infty} A_{i}^{c}\\)\n\nTome \\(\\omega \\in (\\bigcup_{i=1}^{\\infty} A_{i})^{c}\\)\n\\(\\Rightarrow \\omega \\notin \\bigcup_{i=1}^{\\infty} A_{i}\\) (Por definição de complementar)\n\\(\\Rightarrow \\omega \\notin A_{i}, \\forall i=1,2,...\\) (Se não está na união, não está em nenhum conjunto)\n\\(\\Rightarrow \\omega \\in A_{i}^{c}, \\forall i=1,2,...\\) (Por definição de complementar)\n\\(\\Rightarrow \\omega \\in \\bigcap_{i=1}^{\\infty} A_{i}^{c}\\) (Se pertence a todos os complementares, pertence à intersecção deles)\n\nParte 2: \\(\\bigcap_{i=1}^{\\infty} A_{i}^{c} \\subseteq (\\bigcup_{i=1}^{\\infty} A_{i})^{c}\\)\n\nTome \\(\\omega \\in \\bigcap_{i=1}^{\\infty} A_{i}^{c}\\)\n\\(\\Rightarrow \\omega \\in A_{i}^{c}, \\forall i=1,2,...\\) (Por definição de intersecção)\n\\(\\Rightarrow \\omega \\notin A_{i}, \\forall i=1,2,...\\) (Por definição de complementar)\n\\(\\Rightarrow \\omega \\notin \\bigcup_{i=1}^{\\infty} A_{i}\\) (Se não está em nenhum conjunto, não está na união)\n\\(\\Rightarrow \\omega \\in (\\bigcup_{i=1}^{\\infty} A_{i})^{c}\\) (Por definição de complementar)",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução: Da Teoria à Prática Analítica</span>"
    ]
  },
  {
    "objectID": "aula01.html#análise-dinâmica-sequências-de-conjuntos",
    "href": "aula01.html#análise-dinâmica-sequências-de-conjuntos",
    "title": "1  Introdução: Da Teoria à Prática Analítica",
    "section": "1.3 Análise Dinâmica: Sequências de Conjuntos",
    "text": "1.3 Análise Dinâmica: Sequências de Conjuntos\nAgora, voltamos ao nosso problema original: analisar o comportamento dos usuários ao longo do tempo. Para isso, introduzimos o conceito de sequências de conjuntos.\n\nDefinição 1.4 (Sequência Monótona) Uma sequência \\(\\{A_{n}\\}_{n \\ge 1}\\) é dita ser monótona se:\n\n\\(A_{1} \\subseteq A_{2} \\subseteq A_{3} \\subseteq ...\\) (isto é, \\(A_n\\) é não decrescente, denotado por \\(A_n \\uparrow\\)).\n\\(A_{1} \\supseteq A_{2} \\supseteq A_{3} \\supseteq ...\\) (isto é, \\(A_n\\) é não crescente, denotado por \\(A_n \\downarrow\\)).\n\nO limite de uma sequência monótona é denotado por:\n\nse \\(A_n \\uparrow\\), \\(\\lim_{n \\to \\infty} A_{n} = \\bigcup_{i=1}^{\\infty} A_{i}\\).\nse \\(A_n \\downarrow\\), \\(\\lim_{n \\to \\infty} A_{n} = \\bigcap_{i=1}^{\\infty} A_{i}\\).\n\n\n\nPerspectiva de Data Science: Sequências monótonas modelam processos de acumulação ou desgaste.\n\nNão decrescente (\\(A_n \\uparrow\\)): Representa a aquisição cumulativa. Se \\(A_n = \\{\\)usuários que fizeram login pelo menos uma vez até o mês \\(n\\)\\(\\}\\), este conjunto só pode crescer. O limite é o conjunto de todos os usuários que já logaram alguma vez na história.\nNão crescente (\\(A_n \\downarrow\\)): Representa a retenção de uma coorte. Se \\(A_1 = \\{\\)usuários que se cadastraram em Janeiro\\(\\}\\) e \\(A_n = \\{\\)usuários de Janeiro que ainda estavam ativos no mês \\(n\\)\\(\\}\\), este conjunto só pode diminuir. O limite representa os usuários de Janeiro que permaneceram leais para sempre.\n\n\n\nExemplo 1.1 Considere \\(\\Omega = \\mathbb{N}\\) e as sequências:\n\n\\(\\{A_{n}\\}_{n \\ge 1}\\) com \\(A_{n} = \\{1, 2, ..., n\\}\\).\n\\(\\{B_{n}\\}_{n \\ge 1}\\) com \\(B_{n} = \\{2n, 2n+2, 2n+4, ...\\}\\).\n\nLimites de \\(A_n\\) e \\(B_n\\):\n\nNotemos que \\(A_{1}=\\{1\\}, A_{2}=\\{1,2\\},... \\rightarrow A_{1} \\subseteq A_{2} \\subseteq ...\\). Então \\(\\{A_{n}\\}_{n \\ge 1}\\) é monótona não decrescente. Logo, \\(\\lim_{n \\to \\infty} A_{n} = \\bigcup_{i=1}^{\\infty} A_{i} = \\{1\\} \\cup \\{1,2\\} \\cup \\dots = \\mathbb{N} - \\{0\\}\\).\n\\(B_{1}=\\{2,4,6,...\\}, B_{2}=\\{4,6,...\\},... \\Rightarrow B_{1} \\supseteq B_{2} \\supseteq ...\\). A sequência \\(\\{B_{n}\\}_{n \\ge 1}\\) é monótona não crescente. Logo, \\(\\lim_{n \\to \\infty} B_{n} = \\bigcap_{i=1}^{\\infty} B_{i} = \\{2,4,6,...\\} \\cap \\{4,6,...\\} \\cap ... = \\emptyset\\).",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução: Da Teoria à Prática Analítica</span>"
    ]
  },
  {
    "objectID": "aula01.html#comportamento-de-longo-prazo-limite-de-sequências",
    "href": "aula01.html#comportamento-de-longo-prazo-limite-de-sequências",
    "title": "1  Introdução: Da Teoria à Prática Analítica",
    "section": "1.4 Comportamento de Longo Prazo: Limite de Sequências",
    "text": "1.4 Comportamento de Longo Prazo: Limite de Sequências\nMas e o comportamento geral de login, que não é necessariamente monótono? Um usuário pode estar ativo um mês e inativo no outro. É aqui que os conceitos de limite superior e inferior se tornam ferramentas analíticas poderosas para resolver nosso problema.\n\nDefinição 1.5 (Limite Superior e Inferior) Para definir o limite de uma sequência qualquer de conjuntos \\(\\{A_{n}\\}_{n \\ge 1}\\), considere duas sequências auxiliares \\(\\{B_{n}\\}_{n \\ge 1}\\) e \\(\\{C_{n}\\}_{n \\ge 1}\\):\n\\[B_{n} = \\bigcap_{k=n}^{\\infty} A_{k}, \\quad n \\ge 1 \\tag{1.1}\\] \\[C_{n} = \\bigcup_{k=n}^{\\infty} A_{k}, \\quad n \\ge 1 \\tag{1.2}\\]\n\\(B_{1} = A_{1} \\cap A_{2} \\cap A_{3} \\cap ...\\) \\(B_{2} = A_{2} \\cap A_{3} \\cap ...\\)\n\\(C_{1} = A_{1} \\cup A_{2} \\cup A_{3} \\cup ...\\) \\(C_{2} = A_{2} \\cup A_{3} \\cup ...\\)\n\\(\\Rightarrow \\{B_{n}\\}_{n \\ge 1}\\) é uma sequência monótona não decrescente. \\(\\rightarrow \\{C_{n}\\}_{n \\ge 1}\\) é uma sequência monótona não crescente.\n\\(B_n \\subseteq A_n \\subseteq C_n\\).\nDessa forma, como sequências monótonas, seus limites existem:\n\\[\\lim_{n \\to \\infty} B_{n} = \\bigcup_{n=1}^{\\infty} B_{n} = \\bigcup_{n=1}^{\\infty} \\bigcap_{k=n}^{\\infty} A_{k} \\tag{1.3}\\] \\[\\lim_{n \\to \\infty} C_{n} = \\bigcap_{n=1}^{\\infty} C_{n} = \\bigcap_{n=1}^{\\infty} \\bigcup_{k=n}^{\\infty} A_{k} \\tag{1.4}\\]\nCom base nesses limites, podemos definir o comportamento de longo prazo de qualquer sequência \\(\\{A_n\\}\\).\nSe \\(A_1, A_2, ...\\) é uma sequência de conjuntos:\n\nO limite superior da sequência é definido por: \\[\\limsup_{n \\to \\infty} A_{n} = \\bigcap_{n=1}^{\\infty} \\bigcup_{k=n}^{\\infty} A_{k}\\]\nO limite inferior da sequência é definido por: \\[\\liminf_{n \\to \\infty} A_{n} = \\bigcup_{n=1}^{\\infty} \\bigcap_{k=n}^{\\infty} A_{k}\\]\n\n\n\nPerspectiva de Data Science (A Solução): Seja \\(A_n = \\{\\)usuários ativos no mês \\(n\\)\\(\\}\\).\n\nliminf Aₙ (Limite Inferior): É o conjunto dos elementos que pertencem a \\(A_n\\) para todo \\(n\\) a partir de um certo ponto. Este é o conjunto dos usuários leais (hardcore). São aqueles que, após um tempo, se tornam permanentemente ativos.\nlimsup Aₙ (Limite Superior): É o conjunto dos elementos que pertencem a \\(A_n\\) para infinitos valores de \\(n\\). Este é o conjunto dos usuários esporádicos (recorrentes). São aqueles que podem desaparecer, mas sempre acabam voltando.\n\n\n\n1.4.0.1 Interpretação Matemática Rigorosa\nA seguir, demonstramos formalmente por que limsup corresponde à noção de “pertencer a infinitos conjuntos da sequência”.\nConforme a Definição 1.5, se \\(\\omega \\in \\limsup A_n\\), então \\(\\omega \\in \\bigcup_{k=n}^{\\infty} A_k, \\forall n=1,2,...\\)\nEm particular, \\(\\omega \\in C_1 = \\bigcup_{k=1}^{\\infty} A_k\\). Então, \\(\\exists n_1\\) tal que \\(\\omega \\in A_{n_1}\\).\nTambém, \\(\\omega \\in C_{n_1+1} = \\bigcup_{k=n_1+1}^{\\infty} A_k\\). \\(\\Rightarrow \\exists n_2 \\ge n_1+1\\) tal que \\(\\omega \\in A_{n_2}\\).\nProcedendo sempre indutivamente dessa forma, concluímos que existe uma subsequência \\(\\{A_{n_k} : k \\ge 1\\}\\) de tal forma que \\(\\omega \\in A_{n_k}, \\forall k=1,2,...\\)\nReciprocamente, dado \\(\\omega\\) qualquer, suponha que consigamos uma subsequência \\(\\{A_{n_k}\\}_{k \\ge 1}\\) tal que \\(\\omega \\in A_{n_k}, k=1,2,...\\). Dado \\(n\\) positivo, \\(\\exists n_k\\) tal que \\(n_k \\ge n\\). Como \\(\\omega \\in A_{n_k}\\) e \\(n_k \\ge n\\), \\(\\Rightarrow \\omega \\in \\bigcup_{k=n}^{\\infty} A_k\\).\nLogo, \\(\\omega \\in C_n, \\forall n=1,2,... \\rightarrow \\omega \\in \\limsup A_n\\).\nFinalmente, \\(\\omega \\in \\limsup A_n\\) significa existir uma subsequência \\(\\{A_{n_k}\\}_{k \\ge 1}\\) com \\(\\omega \\in A_{n_k}, \\forall k=1,2,...\\).\nPortanto, equivale a \\(\\omega\\) pertencer a infinitos elementos da sequência \\(\\{A_n\\}_{n \\ge 1}\\). Notação: \\(\\{\\limsup A_n\\} = \\{A_n \\text{ infinitas vezes}\\}\\).\n\nDefinição 1.6 (Limite de Sequência de Conjuntos) Dizemos que \\(\\{A_n\\}_{n \\ge 1}\\) tem limite \\(A\\), e escrevemos \\(\\lim_{n \\to \\infty} A_n = A\\), quando: \\[\\liminf_{n \\to \\infty} A_n = \\limsup_{n \\to \\infty} A_n = A\\]\n\n\nNota: Em Data Science, o caso onde liminf = limsup significa que, no longo prazo, o comportamento do sistema se estabiliza. Os usuários esporádicos eventualmente se tornam leais ou desaparecem, e o conjunto de usuários ativos para de flutuar.\n\n\nExemplo 1.2 Seja \\(\\{A_{n}\\}_{n \\ge 1}\\) com \\(A_{n} = [0, \\frac{n}{n+1})\\). Encontre \\(\\lim_{n \\to \\infty} A_n\\).\n\nLimite inferior: \\(\\liminf_{n \\to \\infty} A_{n} = \\bigcup_{n=1}^{\\infty} \\bigcap_{k=n}^{\\infty} A_{k}\\). \\(\\bigcap_{k=n}^{\\infty} A_{k} = \\left[0, \\frac{n}{n+1}\\right) \\cap \\left[0, \\frac{n+1}{n+2}\\right) \\cap \\dots = \\left[0, \\frac{n}{n+1}\\right)\\). \\(\\bigcup_{n=1}^{\\infty} \\left[0, \\frac{n}{n+1}\\right) = [0, 1)\\).\nLimite superior: \\(\\limsup_{n \\to \\infty} A_{n} = \\bigcap_{n=1}^{\\infty} \\bigcup_{k=n}^{\\infty} A_{k}\\). \\(\\bigcup_{k=n}^{\\infty} A_{k} = \\left[0, \\frac{n}{n+1}\\right) \\cup \\left[0, \\frac{n+1}{n+2}\\right) \\cup \\dots = [0, 1)\\). \\(\\bigcap_{n=1}^{\\infty} [0, 1) = [0, 1)\\).\n\nEntão, como vimos na Definição 1.6, \\(\\liminf_{n \\to \\infty} A_{n} = \\limsup_{n \\to \\infty} A_{n} = \\lim_{n \\to \\infty} A_{n} = [0, 1)\\).",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução: Da Teoria à Prática Analítica</span>"
    ]
  },
  {
    "objectID": "aula01.html#implementação-prática-em-r",
    "href": "aula01.html#implementação-prática-em-r",
    "title": "1  Introdução: Da Teoria à Prática Analítica",
    "section": "1.5 Implementação Prática em R",
    "text": "1.5 Implementação Prática em R\nAgora que estabelecemos o formalismo matemático, vamos traduzir esses conceitos para a prática computacional. Usaremos a linguagem R para simular o problema dos usuários de streaming e aplicar as operações de conjuntos para encontrar, de fato, os usuários “leais” e os “esporádicos”.\n\n1.5.1 Operações Básicas com Vetores\nEm R, um vetor de elementos únicos se comporta de maneira análoga a um conjunto. Funções base como union(), intersect() e setdiff() implementam as operações que discutimos.\n\n# Nosso universo de dados: 20 usuários\nOmega &lt;- 1:20\n\n# Segmento A: Usuários do plano Premium\nA &lt;- c(1, 5, 8, 12, 15, 18)\n\n# Segmento B: Usuários que ouviram &gt;100 horas no mês\nB &lt;- c(2, 5, 8, 9, 10, 15, 20)\n\n# Intersecção (A ∩ B): Usuários Premium E que ouviram &gt;100h\nintersect(A, B)\n\n[1]  5  8 15\n\n# União (A U B): Usuários Premium OU que ouviram &gt;100h\nunion(A, B)\n\n [1]  1  5  8 12 15 18  2  9 10 20\n\n# Diferença (A - B): Usuários Premium que NÃO ouviram &gt;100h\nsetdiff(A, B)\n\n[1]  1 12 18\n\n# Complementar (A^c): Usuários que NÃO são Premium\nsetdiff(Omega, A)\n\n [1]  2  3  4  6  7  9 10 11 13 14 16 17 19 20\n\n\n\n\n1.5.2 Analisando o Comportamento de Usuários ao Longo do Tempo\nVamos agora simular 12 meses de atividade para nossos 20 usuários. Criaremos uma lista de conjuntos, An_list, onde An_list[[n]] contém os IDs dos usuários ativos no mês n.\nPara tornar o exemplo claro, vamos criar perfis de usuários específicos: * Usuários Leais (Hardcore): {1, 2}. Estão sempre ativos. * Usuários Esporádicos (Recorrentes): {10, 11}. Ficam ativos em meses pares. * Usuários “Churned” (Desistentes): {18, 19}. Ativos no início, mas somem. * Usuário Novo: {20}. Aparece apenas no final.\n\nset.seed(1)\n\n# Definindo nossos usuários\nleais &lt;- c(1, 2)\nesporadicos &lt;- c(10, 11)\nchurned &lt;- c(18, 19)\nnovo &lt;- 20\noutros_aleatorios &lt;- c(5, 8, 15) # Atividade irregular\n\n# Criando a lista de conjuntos de usuários ativos para 12 meses\nAn_list &lt;- vector(\"list\", 12)\nfor (n in 1:12) {\n  ativos_n &lt;- leais # Leais estão sempre ativos\n  \n  # Esporádicos ativos em meses pares, mas garantimos que não no último mês\n  if (n %% 2 == 0 && n &lt; 12) { \n    ativos_n &lt;- c(ativos_n, esporadicos)\n  }\n  \n  if (n &lt; 6) { # Desistentes\n    ativos_n &lt;- c(ativos_n, churned)\n  }\n  \n  if (n &gt; 9 && n &lt; 12) { # Novo usuário, mas não no último mês\n    ativos_n &lt;- c(ativos_n, novo)\n  }\n  \n  # Atividade aleatória, mas não no último mês\n  if (n &lt; 12) {\n    ativos_n &lt;- c(ativos_n, sample(outros_aleatorios, 1))\n  }\n  \n  An_list[[n]] &lt;- unique(ativos_n)\n}\n\n# Vamos inspecionar os usuários ativos no Mês 2 e Mês 11\nprint(\"Usuários Ativos no Mês 2:\")\n\n[1] \"Usuários Ativos no Mês 2:\"\n\nprint(sort(An_list[[2]]))\n\n[1]  1  2 10 11 15 18 19\n\nprint(\"Usuários Ativos no Mês 11:\")\n\n[1] \"Usuários Ativos no Mês 11:\"\n\nprint(sort(An_list[[11]]))\n\n[1]  1  2 15 20\n\n\n\nNota sobre a Simulação Finita: Os conceitos de liminf e limsup são definidos para sequências infinitas (\\(n \\to \\infty\\)). Ao aplicá-los a uma sequência finita (N=12), surge um “efeito de borda”: o cálculo do liminf é fortemente influenciado pelo último mês da observação, o que pode distorcer a identificação dos usuários verdadeiramente “leais”.\nPara contornar essa limitação e garantir que nosso exemplo prático ilustre corretamente a teoria, ajustamos deliberadamente a simulação. Modelamos o último mês como um período em que o sistema já atingiu um “estado estável”, onde apenas os usuários leais permanecem. Esta não é uma “trapaça”, mas sim uma estratégia de modelagem consciente para emular um comportamento de longo prazo dentro de uma janela de tempo finita, tornando o propósito pedagógico do exemplo mais claro e preciso.\n\n\n\n1.5.3 Calculando liminf e limsup\nCom nossa sequência de conjuntos An_list, podemos agora implementar as definições de liminf e limsup para encontrar nossos perfis de usuários. A função Reduce() é perfeita para aplicar uma operação (como union ou intersect) de forma cumulativa a uma lista de conjuntos.\n\n# Número de meses\nN &lt;- length(An_list)\n\n# --- Cálculo do Limite Superior (Usuários Esporádicos + Leais) ---\n# limsup An = Intersecção(n=1 a N) de [União(k=n a N) de Ak]\n\nCn_list &lt;- vector(\"list\", N)\nfor (n in 1:N) {\n  # União de todos os conjuntos de k=n até o final\n  Cn_list[[n]] &lt;- Reduce(union, An_list[n:N])\n}\nlimsup_An &lt;- Reduce(intersect, Cn_list)\n\nprint(\"Limite Superior (Usuários Leais e Esporádicos):\")\n\n[1] \"Limite Superior (Usuários Leais e Esporádicos):\"\n\nprint(sort(limsup_An))\n\n[1] 1 2\n\n# --- Cálculo do Limite Inferior (Apenas Usuários Leais) ---\n# liminf An = União(n=1 a N) de [Intersecção(k=n a N) de Ak]\n\nBn_list &lt;- vector(\"list\", N)\nfor (n in 1:N) {\n  # Intersecção de todos os conjuntos de k=n até o final\n  Bn_list[[n]] &lt;- Reduce(intersect, An_list[n:N])\n}\nliminf_An &lt;- Reduce(union, Bn_list)\n\nprint(\"Limite Inferior (Apenas Usuários Leais):\")\n\n[1] \"Limite Inferior (Apenas Usuários Leais):\"\n\nprint(sort(liminf_An))\n\n[1] 1 2\n\n\nComo podemos ver, o resultado do código corresponde exatamente à nossa intuição analítica:\n\nO limsup identificou corretamente os usuários que sempre voltam ({1, 2}) e os que aparecem com frequência ({10, 11}).\nO liminf filtrou apenas os usuários que são permanentemente ativos a partir de um certo ponto, ou seja, os verdadeiramente leais ({1, 2}).\n\nEsta seção prática demonstra como a Teoria dos Conjuntos fornece não apenas uma base teórica, mas também um roteiro direto para a implementação de análises de comportamento complexas.",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução: Da Teoria à Prática Analítica</span>"
    ]
  },
  {
    "objectID": "aula02.html",
    "href": "aula02.html",
    "title": "2  Parte 1: Espaço de Probabilidade",
    "section": "",
    "text": "2.1 Um Espaço de Probabilidade\nUm espaço de probabilidade tem três componentes:\nGostaríamos, em princípio, de atribuir probabilidade a qualquer subconjunto de \\(\\Omega\\), isto é, considerar \\(\\mathcal{F}=\\mathcal{P}(\\Omega)\\) (conjunto das partes de \\(\\Omega\\)).\nSe \\(\\Omega\\) for finito ou infinito enumerável, isso é possível. Se \\(\\Omega\\) for não enumerável, por ex. \\(\\Omega=\\mathbb{R}^{2}\\), existem subconjuntos de \\(\\Omega\\) aos quais não é possível “medir” (veja, por ex. Rolla & Lima, Problema da Agulha de Buffon, p. 27).\nIdeia: Restringir \\(\\mathcal{F}\\).\nVamos exigir o seguinte para \\(\\mathcal{F}\\):",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parte 1: Espaço de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula02.html#um-espaço-de-probabilidade",
    "href": "aula02.html#um-espaço-de-probabilidade",
    "title": "2  Parte 1: Espaço de Probabilidade",
    "section": "",
    "text": "Um conjunto \\(\\Omega\\) formado pelos resultados do experimento aleatório (espaço amostral).\nUma classe de subconjuntos de \\(\\Omega\\), denotada aqui por \\(\\mathcal{F}\\), chamados de eventos aleatórios. São os conjuntos de \\(\\mathcal{F}\\) que gostaríamos de “medir” (atribuir probabilidade).\nUma função \\(\\mathbb{P}\\) que associa a cada evento aleatório em \\(\\mathcal{F}\\) um número real, chamada probabilidade ou medida de probabilidade.\n\n\nPerspectiva de Data Science: O Data Frame como Espaço Amostral\nComo podemos traduzir esses conceitos para o nosso dia a dia?\n\n\\(\\Omega\\) (Espaço Amostral): Pense em \\(\\Omega\\) como o seu data frame ou tabela inteira (df). É o universo de todas as observações possíveis que seu experimento (coleta de dados) poderia gerar.\n\\(\\omega\\) (Resultado): Um \\(\\omega\\) (um elemento de \\(\\Omega\\)) é uma única linha do seu data frame. Pode ser um usuário (user_id = 123), uma transação (transaction_id = 9A8B), ou uma sessão de site.\n\\(A\\) (Evento): Um evento \\(A\\) (um subconjunto de \\(\\Omega\\)) é o resultado de uma consulta (query) ou filtro (subset) que você aplica ao seu data frame.\n\nPor exemplo: * df[df$segmento == \"VIP\", ] é o evento \\(A = \\text{\"Clientes VIP\"}\\). * df[df$clicou == 1 & df$pais == \"BR\", ] é o evento \\(B \\cap C\\), onde \\(B = \\text{\"Clicou\"}\\) e \\(C = \\text{\"Brasil\"}\\).\nA Teoria da Probabilidade nos dá as regras formais para calcular o “tamanho” (a probabilidade) desses subconjuntos.\n\n\n\n\n\n\nDefinição 2.3 (\\(\\sigma\\)-álgebra) Dizemos que uma classe \\(\\mathcal{F}\\) de subconjuntos de \\(\\Omega\\) é uma \\(\\sigma\\)-álgebra se \\(\\mathcal{F}\\) satisfaz:\n\n\\(\\Omega \\in \\mathcal{F}\\).\n\\(\\forall A \\in \\mathcal{F} \\Rightarrow A^{c} \\in \\mathcal{F}\\).\nSe \\(A_{1}, A_{2}, ... \\in \\mathcal{F} \\Rightarrow \\bigcup_{n=1}^{\\infty} A_{n} \\in \\mathcal{F}\\).\n\n\n\nPerspectiva de Data Science: A \\(\\sigma\\)-álgebra como o Schema dos Dados\nO conceito de \\(\\sigma\\)-álgebra (\\(\\mathcal{F}\\)) é um dos mais abstratos, mas tem uma analogia prática: pense em \\(\\mathcal{F}\\) como as “perguntas que o seu schema de dados permite fazer”.\nO \\(\\mathcal{F}\\) define quais eventos são “mensuráveis”. Em Data Science, isso é definido pelas features (colunas) que você coletou.\n\nSe seu data frame \\(\\Omega\\) tem colunas idade e genero, sua \\(\\sigma\\)-álgebra \\(\\mathcal{F}\\) permite “medir” (filtrar, agrupar, calcular probabilidades) eventos como “clientes com mais de 30 anos” (\\(A\\)) ou “clientes do gênero feminino” (\\(B\\)).\nAs propriedades da \\(\\sigma\\)-álgebra garantem que você também pode medir combinações:\n\n\nNOT \\(A\\): “clientes com 30 anos ou menos” (\\(A^c\\)).\n\n\n\\(A \\cup B\\): “clientes com mais de 30 anos OU femininos”.\n\n(Prop. 1.1) \\(A \\cap B\\): “clientes femininos com mais de 30 anos”.\n\n\nSe uma feature não foi coletada (ex: renda), eventos como “clientes com renda &gt; 10k” não estão em \\(\\mathcal{F}\\). Não podemos medir sua probabilidade porque essa informação não existe no nosso espaço. A \\(\\sigma\\)-álgebra é, portanto, o conjunto de todas as queries que podemos construir a partir das colunas disponíveis.\n\n\nExemplo 2.3 (Exemplo 1.3) \\(\\Omega \\neq \\emptyset\\) qualquer. \\(\\mathcal{F}=\\mathcal{P}(\\Omega)=\\{A:A\\subseteq\\Omega\\}\\) é \\(\\sigma\\)-algebra (maior \\(\\sigma\\)-algebra).\nDe fato, \\(\\mathcal{P}(\\Omega)\\) é uma \\(\\sigma\\)-álgebra (maior \\(\\sigma\\)-álgebra):\n\n\\(\\Omega \\in \\mathcal{F}=\\mathcal{P}(\\Omega)\\).\n\\(A \\in \\mathcal{F} \\Rightarrow A^{c} \\in \\mathcal{F}\\).\n\\(A_{1}, A_{2}, ... \\in \\mathcal{F} \\Rightarrow \\bigcup_{i=1}^{\\infty} A_{i} \\in \\mathcal{F}\\).\n\n\n\nExemplo 2.4 (Exemplo 1.4) \\(\\Omega \\neq \\emptyset\\) qualquer. \\(\\mathcal{F}=\\{\\emptyset, \\Omega\\}\\) (\\(\\sigma\\)-álgebra trivial).\n\n\nExemplo 2.5 (Exemplo 1.5) \\(\\Omega\\) qualquer e \\(B \\subseteq \\Omega\\). \\(\\mathcal{F}=\\{\\emptyset, \\Omega, B, B^{c}\\}\\) (menor \\(\\sigma\\)-álgebra que contém B).\n\n\nProposição 2.1 (Proposição 1.1) Se \\(\\mathcal{F}\\) é \\(\\sigma\\)-álgebra em \\(\\Omega\\), então:\n\n\\(\\emptyset \\in \\mathcal{F}\\).\n\\(A_{1}, A_{2}, ... \\in \\mathcal{F} \\Rightarrow \\bigcap_{n=1}^{\\infty} A_{n} \\in \\mathcal{F}\\).\nSe \\(A, B \\in \\mathcal{F} \\Rightarrow A-B \\in \\mathcal{F}\\), \\(A-B=A \\cap B^{c}\\).\n\nDemonstração:\nSeja \\(\\mathcal{F}\\) uma \\(\\sigma\\)-álgebra em \\(\\Omega\\).\n\n\\(\\Omega \\in \\mathcal{F} \\Rightarrow \\Omega^{c} \\in \\mathcal{F} \\Rightarrow \\emptyset \\in \\mathcal{F}\\).\n\\(A_{1}, A_{2}, ... \\in \\mathcal{F} \\Rightarrow A_{1}^{c}, A_{2}^{c}, ... \\in \\mathcal{F} \\Rightarrow \\bigcup_{n=1}^{\\infty} A_{n}^{c} \\in \\mathcal{F}\\). \\(\\Rightarrow (\\bigcup_{n=1}^{\\infty} A_{n}^{c})^{c} \\in \\mathcal{F} \\Rightarrow \\bigcap_{n=1}^{\\infty} A_{n} \\in \\mathcal{F}\\).\n\n\n\nProposição 2.2 (Proposição 1.2) Sejam \\(\\mathcal{F}_{1}, \\mathcal{F}_{2}\\) duas \\(\\sigma\\)-álgebras de subconjuntos de \\(\\Omega\\). Então \\(\\mathcal{F}_{1} \\cap \\mathcal{F}_{2}\\) é, também, \\(\\sigma\\)-álgebra de \\(\\Omega\\).\nDemonstração:\nSejam \\(\\mathcal{F}_{1}, \\mathcal{F}_{2}\\) \\(\\sigma\\)-álgebras de subconj. de \\(\\Omega\\).\n\n\\(\\begin{cases} \\Omega \\in \\mathcal{F}_{1} \\\\ e \\\\ \\Omega \\in \\mathcal{F}_{2} \\end{cases} \\Rightarrow \\Omega \\in \\mathcal{F}_{1} \\cap \\mathcal{F}_{2}\\)\n\\(A \\in \\mathcal{F}_{1} \\cap \\mathcal{F}_{2} \\Rightarrow \\begin{cases} A \\in \\mathcal{F}_{1} \\Rightarrow A^{c} \\in \\mathcal{F}_{1} \\\\ A \\in \\mathcal{F}_{2} \\Rightarrow A^{c} \\in \\mathcal{F}_{2} \\end{cases} \\Rightarrow A^{c} \\in \\mathcal{F}_{1} \\cap \\mathcal{F}_{2}\\)\n\\(A_{1}, A_{2}, ... \\in \\mathcal{F}_{1} \\cap \\mathcal{F}_{2}\\), então \\(\\begin{cases} \\bigcup_{n=1}^{\\infty} A_{n} \\in \\mathcal{F}_{1} \\\\ \\bigcup_{n=1}^{\\infty} A_{n} \\in \\mathcal{F}_{2} \\end{cases} \\Rightarrow \\bigcup_{n=1}^{\\infty} A_{n} \\in \\mathcal{F}_{1} \\cap \\mathcal{F}_{2}\\)",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parte 1: Espaço de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula02.html#sigma-algebra-gerada",
    "href": "aula02.html#sigma-algebra-gerada",
    "title": "2  Parte 1: Espaço de Probabilidade",
    "section": "2.2 \\(\\sigma\\)-algebra Gerada",
    "text": "2.2 \\(\\sigma\\)-algebra Gerada\nSeja \\(\\Omega \\neq \\emptyset\\) e \\(\\mathcal{C}\\) uma classe de subconjuntos de \\(\\Omega\\), isto é, \\(\\mathcal{C} \\subseteq \\mathcal{P}(\\Omega)\\).\nPara toda classe \\(\\mathcal{C} \\subseteq \\mathcal{P}(\\Omega)\\), existe ao menos uma \\(\\sigma\\)-álgebra que contém \\(\\mathcal{C}\\), a saber \\(\\mathcal{P}(\\Omega)\\).\nEm geral, seja \\(\\mathcal{C} \\subseteq \\mathcal{P}(\\Omega)\\). Definamos \\(\\mathbb{F}(\\mathcal{C})\\) o conjunto de todas as \\(\\sigma\\)-álgebras de \\(\\Omega\\) que contêm \\(\\mathcal{C}\\).\nNote que \\(\\mathbb{F}(\\mathcal{C}) \\neq \\emptyset\\), pois \\(\\mathcal{P}(\\Omega) \\in \\mathbb{F}(\\mathcal{C})\\).\nDefina \\[\\sigma(\\mathcal{C}) = \\bigcap_{\\mathcal{F} \\in \\mathbb{F}(\\mathcal{C})} \\mathcal{F}\\]\nEntão \\(\\sigma(\\mathcal{C})\\) é a “menor” \\(\\sigma\\)-álgebra que contém \\(\\mathcal{C}\\) (\\(\\sigma\\)-álgebra gerada por \\(\\mathcal{C}\\)).\n\nExemplo 2.6 (Exemplo 1.6) Sejam \\(\\Omega=\\{1,2,3,4,5,6\\}\\) e \\(\\mathcal{C}=\\{\\{1,2,3,4\\},\\{5,6\\}\\}\\).\nTemos que \\(\\mathcal{F}_{1}=\\{\\emptyset, \\Omega, \\{1,2,3,4\\}, \\{5,6\\}\\}\\) e \\(\\mathcal{F}_{2}=\\{\\emptyset, \\Omega, \\{1,2\\}, \\{3,4\\}, \\{5,6\\}, \\{1,2,3,4\\}, \\{1,2,5,6\\}, \\{3,4,5,6\\}\\}\\) são \\(\\sigma\\)-álgebras que contêm \\(\\mathcal{C}\\).\nNote que \\(\\mathcal{F}_{1}\\) é a menor \\(\\sigma\\)-álgebra que contém \\(\\mathcal{C}\\), isto é, \\(\\sigma(\\mathcal{C})\\).\n\n\nExemplo 2.7 (Exemplo 1.7) Sejam \\(\\Omega=\\mathbb{R}\\) e \\(\\mathcal{C}=\\{(-\\infty, x] : x \\in \\mathbb{R}\\}\\).\nA \\(\\sigma(\\mathcal{C})\\) é chamada de \\(\\sigma\\)-álgebra de Borel em \\(\\mathbb{R}\\).\nNotação: \\(\\sigma(\\mathcal{C}) = \\mathcal{B}(\\mathbb{R}) = \\mathcal{B}\\).\nSe \\(B \\in \\mathcal{B}(\\mathbb{R})\\), dizemos que B é um Boreliano. (será o domínio da função de probabilidade).",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parte 1: Espaço de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula02.html#implementação-prática-em-r",
    "href": "aula02.html#implementação-prática-em-r",
    "title": "2  Parte 1: Espaço de Probabilidade",
    "section": "2.3 Implementação Prática em R",
    "text": "2.3 Implementação Prática em R\nNesta seção, ilustramos os conceitos de Espaço Amostral (\\(\\Omega\\)) e Eventos (\\(\\mathcal{F}\\)) usando R. Em ciência de dados, frequentemente lidamos com \\(\\Omega\\) como o conjunto de todas as observações (linhas) em um dataset.\n\n2.3.1 Espaço Amostral Finito: Segmentação de Clientes\nVamos usar o contexto do Exemplo 2.6. Nosso espaço amostral \\(\\Omega\\) é o conjunto de 6 segmentos de clientes.\n\n# Nosso espaço amostral (universo)\nOmega &lt;- 1:6\nprint(Omega)\n\n[1] 1 2 3 4 5 6\n\n\nUm evento \\(A\\) é um subconjunto de \\(\\Omega\\). Por exemplo, o evento “Segmento de Alta Prioridade”.\n\n# Evento A: \"Alta Prioridade\"\nA &lt;- c(1, 2, 3, 4)\n\n# Evento B (complementar de A): \"Baixa Prioridade\"\nB &lt;- c(5, 6)\n\nA \\(\\sigma\\)-álgebra \\(\\mathcal{F}\\) define quais perguntas podemos “fazer” aos nossos dados. A \\(\\sigma\\)-álgebra gerada por \\(\\mathcal{C}=\\{A, B\\}\\) (como no Exemplo 1.6, onde \\(B=A^c\\)) é:\n\\(\\mathcal{F} = \\{\\emptyset, \\Omega, A, B\\}\\)\nVamos verificar as propriedades da \\(\\sigma\\)-álgebra em R:\n\n# (i) Omega está em F\n# (Omega já foi definido)\n\n# (ii) O complemento de um evento em F também está em F\n# A complemento (A^c) em relação a Omega\nA_comp &lt;- setdiff(Omega, A)\nprint(paste(\"Complemento de A:\", list(A_comp)))\n\n[1] \"Complemento de A: 5:6\"\n\n# B complemento (B^c) em relação a Omega\nB_comp &lt;- setdiff(Omega, B)\nprint(paste(\"Complemento de B:\", list(B_comp)))\n\n[1] \"Complemento de B: 1:4\"\n\n# Note que A_comp é B, e B_comp é A. Ambos estão em F.\n\n# (iii) A união de eventos em F está em F\nA_union_B &lt;- union(A, B)\nprint(paste(\"União de A e B:\", list(A_union_B)))\n\n[1] \"União de A e B: c(1, 2, 3, 4, 5, 6)\"\n\n# Note que a união é o próprio Omega, que está em F.\n\n\n\n2.3.2 Eventos como Subconjuntos de um Data Frame\nNa prática, \\(\\Omega\\) é muitas vezes o conjunto de todas as unidades observacionais (ex: todos os usuários, todas as transações). Os eventos são subconjuntos (filtros) desse dataset.\n\n# Criando um Omega (nosso dataset de usuários)\nset.seed(42) # Para reprodutibilidade\nOmega_data &lt;- data.frame(\n  user_id = 1:100,\n  segmento = sample(c(\"VIP\", \"Regular\", \"Novo\"), 100, replace = TRUE),\n  clicou_anuncio = sample(c(0, 1), 100, replace = TRUE)\n)\n\n# head(Omega_data)\n\nAqui, \\(\\Omega\\) é o conjunto dos 100 user_ids.\nVamos definir alguns eventos:\n\n\\(A\\): Usuários que são “VIP”.\n\\(B\\): Usuários que “clicaram no anúncio” (resultado 1).\n\n\n# Evento A: \"Usuário é VIP\"\nA &lt;- subset(Omega_data, segmento == \"VIP\")\ncat(\"Número de elementos em A (VIPs):\", nrow(A), \"\\n\")\n\nNúmero de elementos em A (VIPs): 40 \n\n# Evento B: \"Usuário clicou\"\nB &lt;- subset(Omega_data, clicou_anuncio == 1)\ncat(\"Número de elementos em B (Clicaram):\", nrow(B), \"\\n\")\n\nNúmero de elementos em B (Clicaram): 56 \n\n\nPodemos usar as operações de conjunto (que são a base da \\(\\sigma\\)-álgebra) para definir eventos mais complexos:\n\n# 1. Evento Complemento: A^c (Não-VIPs)\n# A^c = Omega - A\nA_comp &lt;- subset(Omega_data, segmento != \"VIP\")\ncat(\"Número de elementos em A^c (Não-VIPs):\", nrow(A_comp), \"\\n\")\n\nNúmero de elementos em A^c (Não-VIPs): 60 \n\n# 2. Evento Interseção: A ∩ B (Usuários VIP que clicaram)\nA_inter_B &lt;- subset(Omega_data, segmento == \"VIP\" & clicou_anuncio == 1)\n# ou\n# A_inter_B &lt;- merge(A, B)\ncat(\"Número de elementos em A ∩ B (VIPs que clicaram):\", nrow(A_inter_B), \"\\n\")\n\nNúmero de elementos em A ∩ B (VIPs que clicaram): 22 \n\n# 3. Evento União: A U B (Usuários que são VIP ou clicaram)\nA_union_B &lt;- subset(Omega_data, segmento == \"VIP\" | clicou_anuncio == 1)\ncat(\"Número de elementos em A U B (VIPs ou clicaram):\", nrow(A_union_B), \"\\n\")\n\nNúmero de elementos em A U B (VIPs ou clicaram): 74 \n\n\nEste exercício mostra como as operações teóricas de \\(\\sigma\\)-álgebra (complemento, união, interseção) são mapeadas diretamente para as operações de filtro (E/OU/NÃO) que usamos diariamente em R (e SQL) para análise de dados.",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parte 1: Espaço de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula03.html",
    "href": "aula03.html",
    "title": "3  Definição Axiomática de Probabilidade",
    "section": "",
    "text": "3.1 Propriedades\nNa aula anterior, definimos nosso “universo” (\\(\\Omega\\)) e os “eventos” (\\(\\mathcal{F}\\)) como filtros ou queries que podemos aplicar a ele. Na prática, em ciência de dados, costumamos calcular probabilidades usando a intuição de contagem (frequência relativa):\n\\(\\mathbb{P}(A) = \\frac{\\text{Número de linhas que satisfazem A}}{\\text{Número total de linhas em } \\Omega} = \\frac{\\text{nrow}(A)}{\\text{nrow}(\\Omega)}\\)\nIsso funciona bem para datasets finitos. Mas e se \\(\\Omega\\) for infinito? (Ex: “o conjunto de todas as possíveis transações futuras”)? E se \\(\\Omega\\) for contínuo? (Ex: \\(\\Omega = \\mathbb{R}\\), o tempo de resposta de um servidor)? Não podemos mais “contar linhas”.\nPrecisamos de um sistema de regras mais fundamental e robusto, que funcione para qualquer tipo de espaço amostral. Esse sistema é a definição axiomática. Ela define as “regras do jogo” que qualquer função \\(\\mathbb{P}\\) deve obedecer para ser chamada de “probabilidade”.\nNotas:\nSeja \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) um espaço de probabilidade e \\(A, B, A_1, A_2, ... \\in \\mathcal{F}\\). Então:\nDemonstração",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Definição Axiomática de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula03.html#propriedades",
    "href": "aula03.html#propriedades",
    "title": "3  Definição Axiomática de Probabilidade",
    "section": "",
    "text": "\\(\\mathbb{P}(\\emptyset) = 0\\)\n\\(\\mathbb{P}(A^{c}) = 1 - \\mathbb{P}(A)\\) 2’) \\(A_{1}, ..., A_{n}\\) disjuntos \\(\\Rightarrow \\mathbb{P}(\\bigcup_{i=1}^{n} A_{i}) = \\sum_{i=1}^{n} \\mathbb{P}(A_{i})\\)\nSe \\(A \\subseteq B \\Rightarrow \\mathbb{P}(A) \\le \\mathbb{P}(B)\\) e \\(\\mathbb{P}(B-A) = \\mathbb{P}(B) - \\mathbb{P}(A)\\).\n\\(\\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_{n}) \\le \\sum_{n=1}^{\\infty} \\mathbb{P}(A_{n})\\)\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\n\nSeja \\(A_{1} = \\Omega\\) e \\(A_{n} = \\emptyset, \\forall n \\ge 2\\). Então \\(\\Omega = \\bigcup_{n=1}^{\\infty} A_{n}\\). \\(1 = \\mathbb{P}(\\Omega) = \\sum_{n=1}^{\\infty} \\mathbb{P}(A_{n}) = \\mathbb{P}(\\Omega) + \\sum_{n=2}^{\\infty} \\mathbb{P}(\\emptyset) = 1 + \\sum_{n=2}^{\\infty} \\mathbb{P}(\\emptyset)\\). Suponha que \\(\\mathbb{P}(\\emptyset) &gt; 0 \\Rightarrow\\) absurdo. Logo, \\(\\mathbb{P}(\\emptyset) = 0\\). Aqui usamos Ax. 1 e 3.\nSeja \\(A_{1} = A\\), \\(A_{2} = A^{c}\\), \\(A_{n} = \\emptyset, n \\ge 3\\). \\(1 = \\mathbb{P}(\\Omega) = \\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_{n}) = \\sum_{n=1}^{\\infty} \\mathbb{P}(A_{n})\\) \\(= \\mathbb{P}(A) + \\mathbb{P}(A^{c}) + \\sum_{n=3}^{\\infty} \\mathbb{P}(A_{n}) \\Rightarrow 1 = \\mathbb{P}(A) + \\mathbb{P}(A^{c}) + 0\\). \\(\\Rightarrow \\mathbb{P}(A^{c}) = 1 - \\mathbb{P}(A)\\). (Fazer 2’).\nPodemos escrever \\(B = A \\cup (B-A)\\) (união disjunta). \\(\\Rightarrow \\mathbb{P}(B) = \\mathbb{P}(A) + \\mathbb{P}(B-A) \\Rightarrow \\mathbb{P}(B-A) = \\mathbb{P}(B) - \\mathbb{P}(A)\\). Como \\(\\mathbb{P}(B-A) \\ge 0 \\Rightarrow \\mathbb{P}(A) \\le \\mathbb{P}(B)\\).\nVamos escrever \\(\\bigcup_{n=1}^{\\infty} A_{n}\\) como uma união de eventos disjuntos. Seja \\(B_{1} = A_{1}\\) e \\(B_{n} = (\\bigcup_{k=1}^{n-1} A_{k})^{c} \\cap A_{n}, n \\ge 2\\). Então \\(B_n \\subseteq A_n, \\forall n\\)  Portanto, \\[\\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_{n}) = \\mathbb{P}(\\bigcup_{n=1}^{\\infty} B_{n}) = \\sum_{n=1}^{\\infty} \\mathbb{P}(B_{n}) \\le \\sum_{n=1}^{\\infty} \\mathbb{P}(A_{n})\\] (Pois \\(B_n \\subseteq A_n \\Rightarrow \\mathbb{P}(B_n) \\le \\mathbb{P}(A_n)\\) pela prop. 3).\n\n\nPerspectiva de Data Science: Propriedades como “Sanity Checks”\nEssas propriedades são ferramentas de “sanidade” que usamos o tempo todo, muitas vezes sem perceber.\n\nProp 2 (Complemento): \\(\\mathbb{P}(\\text{taxa de churn}) = 1 - \\mathbb{P}(\\text{taxa de retenção})\\). É a base para modelar problemas binários (fraude/não-fraude, clicou/não-clicou).\nProp 3 (Monotonicidade): \\(\\mathbb{P}(\\text{Clicou E Comprou}) \\le \\mathbb{P}(\\text{Clicou})\\). A probabilidade de um evento mais específico (um subconjunto) nunca pode ser maior que a do evento geral. Se seu dashboard mostrar o contrário, há um erro no filtro.\nProp 5 (Inclusão-Exclusão): Essencial para evitar contagem dupla. Se você quer saber a proporção de usuários que são “VIP” ou “clicaram em e-mails” (grupos com sobreposição), você não pode simplesmente somar as duas proporções. Você deve subtrair a interseção: \\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\).\n\n\n\nTeorema 3.1 (Teorema 1.1 (Continuidade da probabilidade)) Seja \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) um espaço de probabilidade. Se \\(\\{A_n\\}_{n \\ge 1} \\in \\mathcal{F}\\) com \\(A_n \\uparrow A\\) ou \\(A_n \\downarrow A\\), e \\(A \\in \\mathcal{F}\\). Então \\[\\lim_{n \\to \\infty} \\mathbb{P}(A_{n}) = \\mathbb{P}(A).\\]\nDemonstração\nConsidere \\(A_n \\uparrow A\\), isto é, \\(A_{1} \\subseteq A_{2} \\subseteq A_{3} \\subseteq ...\\) Note que \\(A = \\bigcup_{n=1}^{\\infty} A_{n}\\). Defina \\(B_{1} = A_{1}\\), \\(B_{n} = A_{n} - A_{n-1}\\), \\(n \\ge 2\\). Os \\(B_k\\) são disjuntos e \\(\\bigcup_{k=1}^{\\infty} B_k = \\bigcup_{k=1}^{\\infty} A_k = A\\).\n\\[\\mathbb{P}(A) = \\mathbb{P}(\\bigcup_{k=1}^{\\infty} A_{k}) = \\mathbb{P}(\\bigcup_{k=1}^{\\infty} B_{k}) = \\sum_{k=1}^{\\infty} \\mathbb{P}(B_{k})\\] \\[= \\lim_{n \\to \\infty} \\sum_{k=1}^{n} \\mathbb{P}(B_{k}) = \\lim_{n \\to \\infty} [ \\mathbb{P}(A_{1}) + \\sum_{k=2}^{n} \\mathbb{P}(A_{k} - A_{k-1}) ]\\] \\[= \\lim_{n \\to \\infty} ( \\mathbb{P}(A_{1}) + [\\mathbb{P}(A_{2}) - \\mathbb{P}(A_{1})] + [\\mathbb{P}(A_{3}) - \\mathbb{P}(A_{2})] + ... + [\\mathbb{P}(A_{n}) - \\mathbb{P}(A_{n-1})] )\\] \\[= \\lim_{n \\to \\infty} \\mathbb{P}(A_{n})\\]",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Definição Axiomática de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula03.html#modelos-probabilisticos",
    "href": "aula03.html#modelos-probabilisticos",
    "title": "3  Definição Axiomática de Probabilidade",
    "section": "3.2 Modelos Probabilisticos",
    "text": "3.2 Modelos Probabilisticos\n\n3.2.1 Modelos Probabilísticos Discretos\nSeja \\(\\Omega=\\{\\omega_{1}, \\omega_{2}, ...\\}\\), \\(\\mathcal{F}=\\mathcal{P}(\\Omega)\\). Considere \\(\\{p_{n}\\}_{n \\ge 1}\\) uma sequência de números reais não negativos, tal que \\[ \\sum_{n=1}^{\\infty} p_{n} = 1. \\]\nBasta definir, para \\(n \\ge 1\\), \\(\\mathbb{P}(\\{\\omega_{n}\\}) = p_{n}\\) e para \\(A \\in \\mathcal{F}\\) \\[ \\mathbb{P}(A) = \\sum_{\\{n \\geq 1 \\: \\omega_{n} \\in A\\}} p_{n} = \\sum_{n=1}^{\\infty} p_n \\mathbb{I}_A(\\omega_n).\\]\n\\(\\mathbb{P}\\) definida dessa forma é uma Probabilidade em \\((\\Omega, \\mathcal{F})\\).\n\n\n3.2.2 Modelos Probabilísticos Contínuos\n\\(\\Omega=\\mathbb{R}\\), \\(\\mathcal{F}=\\mathcal{B}(\\mathbb{R})\\)\nSeja \\(f: \\mathbb{R} \\Rightarrow \\mathbb{R}_{+}\\) tal que \\[ \\int_{-\\infty}^{\\infty} f(t) dt = 1 \\]\nPara \\(A \\in \\mathcal{B}(\\mathbb{R})\\), \\[ \\mathbb{P}(A) = \\int_{A} f(t) dt. \\]",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Definição Axiomática de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula03.html#implementação-prática-em-r",
    "href": "aula03.html#implementação-prática-em-r",
    "title": "3  Definição Axiomática de Probabilidade",
    "section": "3.3 Implementação Prática em R",
    "text": "3.3 Implementação Prática em R\nNa prática, em ciência de dados, trabalhamos com a medida de probabilidade empírica, que é simplesmente a frequência relativa observada em nosso dataset.\nVamos criar um dataset \\(\\:\\Omega\\) e verificar como as propriedades dos axiomas se aplicam a ele.\n\n# Criar nosso universo (Omega)\nset.seed(42)\nOmega_data &lt;- data.frame(\n  user_id = 1:1000,\n  segmento = sample(c(\"VIP\", \"Regular\", \"Novo\"), 1000, replace = TRUE, prob = c(0.1, 0.6, 0.3)),\n  clicou = sample(c(0, 1), 1000, replace = TRUE, prob = c(0.8, 0.2))\n)\n\nhead(Omega_data)\n\n\n\n\n\n\nuser_id\nsegmento\nclicou\n\n\n\n\n1\nVIP\n1\n\n\n2\nVIP\n0\n\n\n3\nRegular\n1\n\n\n4\nNovo\n0\n\n\n5\nNovo\n0\n\n\n6\nRegular\n0\n\n\n\n\n\n\n3.3.1 Definindo nossa Medida de Probabilidade Empírica\nNossa função \\(\\mathbb{P}\\) será baseada na contagem de linhas.\n\n# P(A) = nrow(A) / nrow(Omega)\nP &lt;- function(evento_subset) {\n  nrow(evento_subset) / nrow(Omega_data)\n}\n\n\n\n3.3.2 Verificando os Axiomas e Propriedades\n\n# Axioma 1: P(Omega) = 1\ncat(\"Axioma 1: P(Omega) =\", P(Omega_data), \"\\n\")\n\nAxioma 1: P(Omega) = 1 \n\n# Axioma 2: P(A) &gt;= 0\nA &lt;- subset(Omega_data, segmento == \"VIP\")\ncat(\"Axioma 2: P(A='VIP') =\", P(A), \"(&gt;= 0)\\n\")\n\nAxioma 2: P(A='VIP') = 0.111 (&gt;= 0)\n\n# Axioma 3 (Prop. 2'): Aditividade para eventos disjuntos\nA_vip &lt;- subset(Omega_data, segmento == \"VIP\")\nB_reg &lt;- subset(Omega_data, segmento == \"Regular\")\nA_ou_B &lt;- subset(Omega_data, segmento %in% c(\"VIP\", \"Regular\"))\n\n# Verificando se P(A U B) = P(A) + P(B)\ncat(\"  P(A U B) =\", P(A_ou_B), \"\\n\")\n\n  P(A U B) = 0.734 \n\ncat(\"  P(A) + P(B) =\", P(A_vip) + P(B_reg), \"\\n\")\n\n  P(A) + P(B) = 0.734 \n\n\n\n# Propriedade 2: P(A^c) = 1 - P(A)\nA_clicou &lt;- subset(Omega_data, clicou == 1)\nAc_nao_clicou &lt;- subset(Omega_data, clicou != 1) # ou clicou == 0\n\ncat(\"  P(A^c) =\", P(Ac_nao_clicou), \"\\n\")\n\n  P(A^c) = 0.795 \n\ncat(\"  1 - P(A) =\", 1 - P(A_clicou), \"\\n\")\n\n  1 - P(A) = 0.795 \n\n\n\n# Propriedade 5: P(A U B) = P(A) + P(B) - P(A ∩ B) (Eventos NÃO disjuntos)\n# A = \"Usuário é VIP\"\n# B = \"Usuário Clicou\"\nA &lt;- subset(Omega_data, segmento == \"VIP\")\nB &lt;- subset(Omega_data, clicou == 1)\n\n# A U B (União: VIP OU Clicou)\nA_ou_B_nao_disj &lt;- subset(Omega_data, segmento == \"VIP\" | clicou == 1)\n\n# A ∩ B (Interseção: VIP E Clicou)\nA_e_B &lt;- subset(Omega_data, segmento == \"VIP\" & clicou == 1)\n\ncat(\"  Lado Esquerdo: P(A U B) =\", P(A_ou_B_nao_disj), \"\\n\")\n\n  Lado Esquerdo: P(A U B) = 0.297 \n\ncat(\"  Lado Direito: P(A) + P(B) - P(A ∩ B) =\", P(A) + P(B) - P(A_e_B), \"\\n\")\n\n  Lado Direito: P(A) + P(B) - P(A ∩ B) = 0.297 \n\n\nIsso demonstra que a fórmula de Inclusão-Exclusão é necessária para evitar a contagem dupla de usuários que estão em ambos os grupos.",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Definição Axiomática de Probabilidade</span>"
    ]
  },
  {
    "objectID": "aula04.html",
    "href": "aula04.html",
    "title": "4  Probabilidade Condicional",
    "section": "",
    "text": "4.1 Independência de Eventos\nEm ciência de dados, raramente olhamos para a população inteira (\\(\\Omega\\)). Nosso trabalho é quase sempre “fatiar” os dados para entender subpopulações.\nEm todos esses casos, não estamos perguntando \\(\\mathbb{P}(A)\\), mas sim \\(\\mathbb{P}(A|B)\\). Estamos pedindo a probabilidade de \\(A\\) (conversão, churn, fraude) depois de reduzirmos nosso universo \\(\\Omega\\) para um subconjunto \\(B\\) (usuários do Facebook, clientes VIP, transações atípicas). A probabilidade condicional é a formalização matemática dessa “redução de universo” ou “filtro”.\nNotas:\nDizemos que \\(A_1, A_2, \\dots \\in \\mathcal{F}\\) formam uma partição de \\(\\Omega\\) se são (mutuamente) disjuntos e \\(\\bigcup_{k=1}^{\\infty} A_k = \\Omega\\).\nDessa forma, para todo evento \\(B \\subseteq \\Omega\\), \\[ B = \\bigcup_{k=1}^{\\infty} (A_k \\cap B) \\]\nUsando o Teorema 4.2, podemos obter a fórmula de Bayes: \\[ \\mathbb{P}(A_i|B) = \\frac{\\mathbb{P}(A_i \\cap B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(A_i) \\mathbb{P}(B|A_i)}{\\sum_{k=1}^{\\infty} \\mathbb{P}(A_k) \\mathbb{P}(B|A_k)} \\]\nNotas:\nExercício 19 (lista 2) \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) espaço de probabilidade, \\(A \\in \\mathcal{F}\\), com \\(\\mathbb{P}(A) \\in (0, 1)\\). A e B são independentes se, e só se, \\(\\mathbb{P}(B|A) = \\mathbb{P}(B|A^c)\\).",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilidade Condicional</span>"
    ]
  },
  {
    "objectID": "aula04.html#independência-de-eventos",
    "href": "aula04.html#independência-de-eventos",
    "title": "4  Probabilidade Condicional",
    "section": "",
    "text": "Definição 4.2 (Definição 1.6 (Independência)) Seja \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) um espaço de probabilidade. Os eventos A e B são independentes se, e somente se, \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B) \\]\n\n\n\nEventos de probabilidade zero ou um são independentes de qualquer outro.\nDois eventos são independentes se, e somente se, \\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\).\nSe \\(A \\cap B = \\emptyset,\\) então A e B não são independentes (a menos que um deles tenha probabilidade zero).\n\n\nProposição 4.1 (Proposição 1.3) A é independente de si mesmo se, e somente se, \\(\\mathbb{P}(A)=0\\) ou \\(\\mathbb{P}(A)=1\\).\nDemonstração: De fato, \\(\\mathbb{P}(A \\cap A) = \\mathbb{P}(A) \\mathbb{P}(A) \\Leftrightarrow \\mathbb{P}(A) = [\\mathbb{P}(A)]^2\\) \\(\\Leftrightarrow \\mathbb{P}(A) = 0 \\text{ ou } \\mathbb{P}(A) = 1\\).\n\n\nProposição 4.2 (Proposição 1.4) Se A e B são independentes, então A e \\(B^c\\) também são independentes. (e também \\(A^c\\) e B, e ainda \\(A^c\\) e \\(B^c\\)).\nDemonstração: \\(A \\text{ e } B \\text{ indep.} \\Rightarrow \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\mathbb{P}(B)\\) Temos \\(A = (A \\cap B) \\cup (A \\cap B^c)\\). \\[ \\mathbb{P}(A \\cap B^c) = \\mathbb{P}(A - (A \\cap B)) = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) \\] \\[ = \\mathbb{P}(A) - \\mathbb{P}(A)\\mathbb{P}(B) \\] \\[ = \\mathbb{P}(A) [1 - \\mathbb{P}(B)] = \\mathbb{P}(A) \\mathbb{P}(B^c). \\]\n\n\nDefinição 4.3 (Definição 1.7) Os eventos aleatórios \\(\\{A_i\\}_{i \\in I}\\) são independentes 2 a 2 se \\[ \\mathbb{P}(A_i \\cap A_j) = \\mathbb{P}(A_i) \\mathbb{P}(A_j), \\quad \\forall i, j \\in I, i \\neq j \\]\n\n\nDefinição 4.4 (Definição 1.8) Os eventos aleatórios \\(\\{A_i\\}_{i \\in I}\\) são (mutuamente) independentes se, e só se, dado qualquer conjunto de índices distintos \\(i_1, \\dots, i_n \\in I\\), \\(n \\ge 2\\), vale \\[ \\mathbb{P}(A_{i_1} \\cap A_{i_2} \\cap \\dots \\cap A_{i_n}) = \\prod_{k=1}^{n} \\mathbb{P}(A_{i_k}) \\]\n\n\nExemplo 4.2 (Exemplo 1.14) Seja \\(\\Omega = \\{u_1, u_2, u_3, u_4\\}\\) (4 usuários), com \\(\\mathbb{P}(\\{u_i\\}) = 1/4, \\forall i\\). Defina os eventos (características dos usuários):\n\n\\(A\\) = “Engajamento Alto” = \\(\\{u_2, u_4\\}\\)\n\\(B\\) = “Região Sudeste” = \\(\\{u_1, u_2\\}\\)\n\\(C\\) = “Acesso via Mobile” = \\(\\{u_1, u_4\\}\\)\n\nVerifique que A, B e C são 2 a 2 independentes, mas não coletivamente independentes.\n\\(\\mathbb{P}(A) = 2/4 = 1/2\\) \\(\\mathbb{P}(B) = 2/4 = 1/2\\) \\(\\mathbb{P}(C) = 2/4 = 1/2\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(\\{u_2\\}) = 1/4\\). Logo \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)\\). (A e B são indep.)\n\\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(\\{u_1\\}) = 1/4\\). Logo \\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(B)\\mathbb{P}(C)\\). (B e C são indep.)\n\\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(\\{u_4\\}) = 1/4\\). Logo \\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(A)\\mathbb{P}(C)\\). (A e C são indep.)\nMas: \\(\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(\\emptyset) = 0\\).\n\\(\\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C) = (1/2)^3 = 1/8\\).\nComo \\(0 \\neq 1/8\\), os eventos não são mutuamente independentes. Saber B e C juntos nos diz que o usuário é \\(u_1\\), o que exclui a possibilidade de ser A.\n\n\nExemplo 4.3 (Exemplo 1.15) Mostre que se \\(A_1, A_2, \\dots, A_n\\) são independentes e \\(\\bigcup_{i=1}^n A_i = \\Omega\\), então \\(\\exists i\\) tal que \\(\\mathbb{P}(A_i) = 1\\).\nDemonstração: \\(\\bigcup_{i=1}^n A_i = \\Omega \\Rightarrow \\mathbb{P}(\\bigcup_{i=1}^n A_i) = 1\\). Pela Lei de De Morgan: \\(1 = \\mathbb{P}((\\bigcap_{i=1}^n A_i^c)^c) = 1 - \\mathbb{P}(\\bigcap_{i=1}^n A_i^c)\\)\n\\(\\Rightarrow \\mathbb{P}(\\bigcap_{i=1}^n A_i^c) = 0\\).\n\\(A_i\\) independentes \\(\\Rightarrow A_i^c\\) independentes \\(\\Rightarrow \\prod_{i=1}^n \\mathbb{P}(A_i^c) = 0\\).\nEntão, \\(\\exists i\\) tal que \\(\\mathbb{P}(A_i^c) = 0 \\Rightarrow \\mathbb{P}(A_i) = 1\\).\n\n\n\n\nPerspectiva de Data Science: A Hipótese Nula de um Teste A/B\nEste exercício é a definição matemática da hipótese nula (\\(H_0\\)) em um teste A/B.\n\n\\(B\\) = O evento de interesse (ex: “Compra”)\n\\(A\\) = “Usuário viu a Versão A (Tratamento)”\n\\(A^c\\) = “Usuário viu a Versão B (Controle)”\n\nA afirmação \\(\\mathbb{P}(B|A) = \\mathbb{P}(B|A^c)\\) se traduz em: “A taxa de compra no grupo Tratamento é exatamente igual à taxa de compra no grupo Controle.”\nO exercício prova que isso é matematicamente equivalente a dizer que “fazer a compra” (\\(B\\)) é independente do “grupo que o usuário viu” (\\(A\\)).\nNosso objetivo no teste é justamente tentar rejeitar essa afirmação, provando que os eventos são dependentes e que \\(\\mathbb{P}(B|A) \\neq \\mathbb{P}(B|A^c)\\).",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilidade Condicional</span>"
    ]
  },
  {
    "objectID": "aula04.html#implementação-prática-em-r",
    "href": "aula04.html#implementação-prática-em-r",
    "title": "4  Probabilidade Condicional",
    "section": "4.2 Implementação Prática em R",
    "text": "4.2 Implementação Prática em R\nVamos usar o dataset da aula anterior para verificar esses conceitos na prática.\n\n# Criar nosso universo (Omega)\nset.seed(42)\nOmega_data &lt;- data.frame(\n  user_id = 1:1000,\n  segmento = sample(c(\"VIP\", \"Regular\", \"Novo\"), 1000, replace = TRUE, prob = c(0.1, 0.6, 0.3)),\n  clicou = sample(c(0, 1), 1000, replace = TRUE, prob = c(0.8, 0.2))\n)\n\n# Definindo nossa função de probabilidade empírica\nP &lt;- function(evento_subset) {\n  nrow(evento_subset) / nrow(Omega_data)\n}\n\nQual a probabilidade de um usuário “clicar” (\\(A\\)) dado que ele é “VIP” (\\(B\\))?\n\n# P(A|B) = P(A ∩ B) / P(B)\n\nlibrary(tidyverse)\n\n# Eventos\nA_clicou &lt;- Omega_data |&gt; filter(clicou == 1)\nB_vip &lt;- Omega_data |&gt; filter(segmento == \"VIP\")\nA_e_B &lt;- Omega_data |&gt; filter(clicou == 1 & segmento == \"VIP\")\n\n# Probabilidades\nP_A_e_B &lt;- P(A_e_B)\nP_B &lt;- P(B_vip)\n\nP_A_dado_B &lt;- P_A_e_B / P_B\ncat(\"P(Clicou | VIP) via fórmula:\", P_A_dado_B, \"\\n\")\n\nP(Clicou | VIP) via fórmula: 0.1711712 \n\n\n\n# Forma Direta (Reduzindo o Universo)\n# 1. Reduzimos nosso universo para Omega' = B_vip\nOmega_filtrado &lt;- Omega_data |&gt; filter(segmento == \"VIP\")\n\n# 2. Calculamos a probabilidade de A dentro desse NOVO universo\n# Note que aqui o denominador é nrow(Omega_filtrado)\nP_A_no_universo_B &lt;- sum(Omega_filtrado$clicou == 1) / nrow(Omega_filtrado)\n\ncat(\"P(Clicou | VIP) via filtro direto:\", P_A_no_universo_B, \"\\n\")\n\nP(Clicou | VIP) via filtro direto: 0.1711712 \n\n\nOs resultados são idênticos, validando a definição.\nOs eventos \\(A\\)=“clicou” e \\(B\\)=“VIP” são independentes? Verificamos se \\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\times \\mathbb{P}(B)\\).\n\nP_A &lt;- P(A_clicou)\n# P_B e P_A_e_B já foram calculados\n\ncat(\"Verificando Independência (Clicou e VIP):\\n\")\n\nVerificando Independência (Clicou e VIP):\n\ncat(\"  Lado Esquerdo: P(A ∩ B) =\", P_A_e_B, \"\\n\")\n\n  Lado Esquerdo: P(A ∩ B) = 0.019 \n\ncat(\"  Lado Direito: P(A) * P(B) =\", P_A * P_B, \"\\n\")\n\n  Lado Direito: P(A) * P(B) = 0.022755 \n\n\nComo os valores são diferentes, os eventos NÃO são independentes. Saber que um usuário é “VIP” MUDA a probabilidade de ele “clicar”.\nA partição do nosso universo são os segmentos: \\(A_1\\)=“VIP”, \\(A_2\\)=“Regular”, \\(A_3\\)=“Novo”. O evento \\(B\\) é “clicou”.\nVamos “remontar” a \\(\\mathbb{P}(B)\\) geral usando a fórmula: \\(\\mathbb{P}(B) = \\mathbb{P}(B|A_1)\\mathbb{P}(A_1) + \\mathbb{P}(B|A_2)\\mathbb{P}(A_2) + \\mathbb{P}(B|A_3)\\mathbb{P}(A_3)\\)\n\n# 1. Probabilidades da Partição P(A_k)\nP_A1_vip &lt;- P(Omega_data |&gt; filter(segmento == \"VIP\"))\nP_A2_reg &lt;- P(Omega_data |&gt; filter(segmento == \"Regular\"))\nP_A3_nov &lt;- P(Omega_data |&gt; filter(segmento == \"Novo\"))\n\n# 2. Probabilidades Condicionais P(B | A_k)\n# Usando o método direto (filtrado)\nP_B_dado_A1 &lt;- Omega_data |&gt; \n  filter(segmento == \"VIP\") |&gt; \n  summarise(media = mean(clicou)) |&gt; \n  pull(media)\n\nP_B_dado_A2 &lt;- Omega_data |&gt; \n  filter(segmento == \"Regular\") |&gt; \n  summarise(media = mean(clicou)) |&gt; \n  pull(media)\n\nP_B_dado_A3 &lt;- Omega_data |&gt; \n  filter(segmento == \"Novo\") |&gt; \n  summarise(media = mean(clicou)) |&gt; \n  pull(media)\n\ncat(\"P(Clicou|VIP):\", P_B_dado_A1, \"\\n\")\n\nP(Clicou|VIP): 0.1711712 \n\ncat(\"P(Clicou|Regular):\", P_B_dado_A2, \"\\n\")\n\nP(Clicou|Regular): 0.2054575 \n\ncat(\"P(Clicou|Novo):\", P_B_dado_A3, \"\\n\")\n\nP(Clicou|Novo): 0.2180451 \n\n# 3. Média Ponderada\nP_B_calculada &lt;- (P_B_dado_A1 * P_A1_vip) + \n                 (P_B_dado_A2 * P_A2_reg) + \n                 (P_B_dado_A3 * P_A3_nov)\n\ncat(\"P(B) via Lei Total (Média Ponderada):\", P_B_calculada, \"\\n\")\n\nP(B) via Lei Total (Média Ponderada): 0.205 \n\n# 4. Probabilidade Real (Geral)\nP_B_real &lt;- P(Omega_data |&gt; filter(clicou == 1))\ncat(\"P(B) Real (Taxa Geral de Clique):\", P_B_real, \"\\n\")\n\nP(B) Real (Taxa Geral de Clique): 0.205 \n\n\nOs valores são idênticos, validando a Lei da Probabilidade Total.",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probabilidade Condicional</span>"
    ]
  },
  {
    "objectID": "aula05.html",
    "href": "aula05.html",
    "title": "5  Exercícios",
    "section": "",
    "text": "Nesta seção, vamos resolver exercícios que parecem puramente teóricos (e são!). Em vez de calcular a probabilidade de “cliques” ou “segmentos”, vamos provar propriedades fundamentais da própria função \\(\\mathbb{P}\\).\nPor que um cientista de dados faria isso? Porque essas propriedades são as garantias e testes de sanidade que rodam “por baixo dos panos” em toda análise. Elas são a justificativa teórica que nos permite:\n\nConfiar que, se um modelo é 99,99% preciso em n cenários, ele continua sendo confiável.\nEntender o que acontece quando combinamos features (interseções) ou segmentos (uniões).\nUsar a lógica (SE/ENTÃO) para criar regras de negócio a partir dos dados.\n\nVamos ver como.\n\nSeja \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) um espaço de probabilidade e \\(\\{A_n\\}_{n \\ge 1} \\in \\mathcal{F}\\). Mostre que:\n\nSe \\(\\mathbb{P}(A_n) = 0, \\forall n=1, 2, \\dots\\), então \\(\\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n) = 0\\).\n\nSolução: Pela Propriedade 4 (Subaditividade), sabemos que \\(\\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n) \\le \\sum_{n=1}^{\\infty} \\mathbb{P}(A_n)\\). Substituindo os valores dados: \\[ \\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n) \\le \\sum_{n=1}^{\\infty} 0 = 0 \\] Como a probabilidade não pode ser negativa (Axioma 2), Logo, \\(\\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n) = 0\\).\n\nSe \\(\\mathbb{P}(A_n) = 1, \\forall n=1, 2, \\dots\\), então \\(\\mathbb{P}(\\bigcap_{n=1}^{\\infty} A_n) = 1\\).\n\nSolução: Usando a Lei de De Morgan e a Propriedade 2: \\[ \\mathbb{P}(\\bigcap_{n=1}^{\\infty} A_n) = 1 - \\mathbb{P}((\\bigcap_{n=1}^{\\infty} A_n)^c) = 1 - \\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n^c) \\] Se \\(\\mathbb{P}(A_n) = 1\\), então \\(\\mathbb{P}(A_n^c) = 1 - \\mathbb{P}(A_n) = 0, \\forall n\\). Pelo item (a), se \\(\\mathbb{P}(A_n^c) = 0, \\forall n\\), então \\(\\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n^c) = 0\\). Substituindo na equação: \\[ \\mathbb{P}(\\bigcap_{n=1}^{\\infty} A_n) = 1 - 0 = 1 \\]\nSeja \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) um espaço de probabilidade e \\(\\{A_n\\}_{n \\ge 1} \\in \\mathcal{F}, \\{B_n\\}_{n \\ge 1} \\in \\mathcal{F}\\). com \\(\\lim_{n \\to \\infty} \\mathbb{P}(A_n) = 1\\) e \\(\\lim_{n \\to \\infty} \\mathbb{P}(B_n) = p\\). Mostre que \\(\\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cap B_n) = p\\).\nSolução: Sabemos que \\(A_n \\subseteq A_n \\cup B_n \\Rightarrow \\mathbb{P}(A_n) \\le \\mathbb{P}(A_n \\cup B_n)\\). Como \\(\\mathbb{P}(A_n \\cup B_n) \\le 1\\), temos: \\[ \\mathbb{P}(A_n) \\le \\mathbb{P}(A_n \\cup B_n) \\le 1 \\]\n\\[ 1=\\lim_{n \\to \\infty} \\mathbb{P}(A_n) \\le \\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cup B_n) \\le 1 \\] \\[ \\Rightarrow \\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cup B_n) = 1 \\] Usando a fórmula da união (Propriedade 5): \\[ \\mathbb{P}(A_n \\cup B_n) = \\mathbb{P}(A_n) + \\mathbb{P}(B_n) - \\mathbb{P}(A_n \\cap B_n) \\] Aplicando o limite em todos os termos: \\[ \\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cup B_n) = \\lim_{n \\to \\infty} \\mathbb{P}(A_n) + \\lim_{n \\to \\infty} \\mathbb{P}(B_n) - \\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cap B_n) \\] Substituindo os valores conhecidos: \\[ 1 = 1 + p - \\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cap B_n) \\] \\[ 0 = p - \\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cap B_n) \\] Logo, \\(\\lim_{n \\to \\infty} \\mathbb{P}(A_n \\cap B_n) = p\\).\nSeja \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) um espaço de probabilidade, \\(A \\in \\mathcal{F}, B \\in \\mathcal{F}\\) tal que \\(\\mathbb{P}(A) &lt; 1\\), \\(\\mathbb{P}(B) &gt; 0\\) e \\(\\mathbb{P}(A|B) = 1\\). Então \\(\\mathbb{P}(B^c|A^c) = 1\\).\nSolução: \\[\n\\begin{align*}\n\\mathbb{P}(B^c|A^c) &= 1 - \\mathbb{P}(B|A^c) \\\\\n                   &= 1 - \\frac{\\mathbb{P}(B \\cap A^c)}{\\mathbb{P}(A^c)} \\\\\n                   &= 1 - \\frac{\\mathbb{P}(A^c|B) \\mathbb{P}(B)}{\\mathbb{P}(A^c)} \\\\\n                   &= 1 - \\frac{(1 - \\mathbb{P}(A|B)) \\mathbb{P}(B)}{\\mathbb{P}(A^c)} \\\\\n\\end{align*}\n\\] Dado que \\(\\mathbb{P}(A|B) = 1\\): \\[\n\\begin{align*}\n\\mathbb{P}(B^c|A^c) &= 1 - \\frac{(1 - 1) \\mathbb{P}(B)}{\\mathbb{P}(A^c)} \\\\\n                   &= 1 - \\frac{0 \\cdot \\mathbb{P}(B)}{\\mathbb{P}(A^c)} \\\\\n                   &= 1 - 0 = 1\n\\end{align*}\n\\] (Note que \\(\\mathbb{P}(A^c) = 1 - \\mathbb{P}(A) &gt; 0\\), pois \\(\\mathbb{P}(A) &lt; 1\\), então a divisão é válida).\nUma moeda com probabilidade \\(p\\) de cara em cada lançamento é lançada infinitas vezes, de maneira independente. Sejam \\(A_n\\): ocorre pelo menos uma cara nos \\(n\\) primeiros lançamentos, \\(n \\ge 1\\). \\(A\\): ocorre pelo menos uma cara.\n\nMostre que \\(A_n \\uparrow A\\).\nMostre que \\(\\mathbb{P}(A) = \\begin{cases} 1, \\text{ se } 0 &lt; p \\le 1 \\\\ 0, \\text{ se } p = 0 \\end{cases}\\).\n\nSolução (a): \\(A_n \\subseteq A_{n+1}, n \\ge 1. \\rightarrow \\{A_n\\}_{n \\ge 1}\\) é uma seq. monótona não decrescente.\nLogo, \\(\\lim_{n \\to \\infty} A_n = \\bigcup_{n=1}^{\\infty} A_n\\).\n\\(\\omega \\in \\bigcup_{n=1}^{\\infty} A_n \\Rightarrow \\exists n \\text{ tal que } \\omega \\in A_n \\Rightarrow \\omega \\in A\\). Também, \\(\\omega \\in A \\Rightarrow \\exists n \\text{ tal que } \\omega \\in A_n \\Rightarrow \\omega \\in \\bigcup_{n=1}^{\\infty} A_n\\).\nSolução (b): (em que \\(E_i\\): ocorreu cara no lançamento \\(i\\)) \\[\n\\begin{align*}\n\\mathbb{P}(A) &= \\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n) = \\mathbb{P}(\\lim_{n \\to \\infty} A_n) = \\lim_{n \\to \\infty} \\mathbb{P}(A_n) = \\lim_{n \\to \\infty} [1 - \\mathbb{P}(A_n^c)] \\\\\n              &= \\lim_{n \\to \\infty} [1 - \\mathbb{P}(E_1^c \\cap E_2^c \\cap \\dots \\cap E_n^c)] \\\\\n              &= \\lim_{n \\to \\infty} [1 - \\prod_{i=1}^n \\mathbb{P}(E_i^c)] \\quad \\text{(indep.)} \\\\\n              &= \\lim_{n \\to \\infty} (1 - (1-p)^n) = \\begin{cases} 1, & \\text{ se } 0 &lt; p \\le 1 \\\\ 0, & \\text{ se } p = 0 \\end{cases}\n\\end{align*}\n\\]\nEm que \\(E_i:\\) ocorreu cara no lançamento \\(i.\\)\n\\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) espaço de probabilidade com \\(A, B, A_1, A_2, \\dots \\in \\mathcal{F}\\). Suponha \\(A_n \\uparrow A\\) e que B é independente de \\(A_n, \\forall n \\ge 1\\). Prove que A e B são independentes.\nSolução: \\[\n\\mathbb{P}(A|B) = \\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n | B) = \\lim_{n \\to \\infty} \\mathbb{P}(A_n|B) = \\lim_{n \\to \\infty} \\mathbb{P}(A_n) = \\mathbb{P}(\\bigcup_{n=1}^{\\infty} A_n) = \\mathbb{P}(A)\n\\] Logo, A e B são independentes.",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exercícios</span>"
    ]
  },
  {
    "objectID": "aula06.html",
    "href": "aula06.html",
    "title": "6  Variáveis Aleatórias",
    "section": "",
    "text": "6.1 Espaço de Probabilidade Induzido\nAté agora, lidamos com o espaço amostral \\(\\Omega\\) (ex: o conjunto de todos os usuários) e eventos \\(\\mathcal{F}\\) (ex: “o usuário é VIP”). No entanto, em Data Science, raramente trabalhamos com o evento “bruto” \\((\\omega)\\).\nNão analisamos o log JSON inteiro de um usuário; nós extraímos métricas numéricas dele:\nUma Variável Aleatória (v.a.) é a ferramenta matemática formal que representa essa extração. É uma função que mapeia o resultado complexo de um experimento (\\(\\omega\\)) para um número real (\\(\\mathbb{R}\\)) que podemos medir e modelar.\nEm todo caso, X é uma função de \\(\\Omega\\) em \\(\\mathbb{R}\\), isto é, \\(X: \\Omega \\rightarrow \\mathbb{R}\\).\nEm todo caso, X é uma função de \\(\\Omega\\) em \\(\\mathbb{R}\\), isto é, \\(X: \\Omega \\rightarrow \\mathbb{R}\\).\nVamos impor uma restrição sobre a função X que permitirá atribuir probabilidades a eventos como “número de caras é, no máximo, 5”.\nNotas:\nDado um espaço de probabilidade \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) e uma v.a. X, definimos o espaço de probabilidade induzido por X como \\((\\mathbb{R}, \\mathcal{B}, \\mathbb{P}_X)\\), em que \\(\\mathcal{B}\\) é a \\(\\sigma\\)-álgebra de Borel em \\(\\mathbb{R}\\), que contém todos os subconjuntos de \\(\\mathbb{R}\\) que nos interessam, e \\[ \\mathbb{P}_X(B) = \\mathbb{P}(\\{\\omega \\in \\Omega : X(\\omega) \\in B\\}), \\quad B \\in \\mathcal{B} \\] \\[ = \\mathbb{P}(X \\in B) \\]\nNotas:",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variáveis Aleatórias</span>"
    ]
  },
  {
    "objectID": "aula06.html#espaço-de-probabilidade-induzido",
    "href": "aula06.html#espaço-de-probabilidade-induzido",
    "title": "6  Variáveis Aleatórias",
    "section": "",
    "text": "Neste caso, o espaço amostral é \\(\\mathbb{R}\\), os eventos aleatórios são os borelianos e a medida de probabilidade é \\(\\mathbb{P}_X\\).\nA medida de probabilidade \\(\\mathbb{P}_X\\) é chamada distribuição de X.\nVerifique que \\(\\mathbb{P}_X\\) é, de fato, uma probabilidade.\n\n\nPerspectiva de Data Science: O Histograma é a Distribuição Induzida \\(\\mathbb{P}_X\\)\nEste é o passo mais importante da Análise Exploratória de Dados (EDA).\n\nNós jogamos \\(\\Omega\\) fora: Não nos importamos mais com os milhões de logs brutos (\\(\\Omega\\)).\nNós focamos em \\(\\mathbb{P}_X\\): Nós nos importamos apenas com a distribuição da feature \\(X\\).\n\nQuando você plota um histograma da session_duration, você está plotando uma estimativa empírica da distribuição induzida \\(\\mathbb{P}_X\\).\nO histograma da session_duration (\\(X\\)) vive no espaço \\((\\mathbb{R}, \\mathcal{B}, \\mathbb{P}_X)\\). Ele não vive mais no espaço de logs brutos (\\(\\Omega, \\mathcal{F}, \\mathbb{P}\\)). A v.a. \\(X\\) nos permitiu “transportar” a probabilidade de um espaço complexo (logs) para um espaço simples (\\(\\mathbb{R}\\)) onde podemos fazer estatística.\n\n\nExemplo 6.8 (Exemplo 2.6) Um experimento coleta 3 features de um usuário: \\(F_1, F_2, F_3\\) (ex: device_type, country, time_of_day). \\(\\Omega = \\{1, \\dots, 6\\} \\times \\{1, \\dots, 10\\} \\times \\{1, \\dots, 24\\}\\) (um espaço amostral com \\(6 \\times 10 \\times 24 = 1440\\) resultados possíveis). \\(\\mathcal{F} = \\mathcal{P}(\\Omega)\\) e \\(\\mathbb{P}\\) é uniforme por simplicidade.\nEstamos interessados apenas na feature \\(X = F_1\\) (device_type, com 6 tipos). \\(X: \\Omega \\rightarrow \\mathbb{R}\\) com \\(X(\\omega) = \\omega_1\\), em que \\(\\omega = (\\omega_1, \\omega_2, \\omega_3) \\in \\Omega\\). Suponha que queiramos calcular \\(\\mathbb{P}(1.5 \\le X \\le 3.4)\\) (ou seja, \\(\\mathbb{P}(X=2 \\text{ ou } X=3)\\)).\nCálculo no espaço original \\((\\Omega)\\): \\[ \\mathbb{P}(\\{\\omega : 1.5 \\le X(\\omega) \\le 3.4\\}) = \\mathbb{P}(\\{\\omega : \\omega_1 \\in \\{2, 3\\}\\}) \\] \\[ = \\frac{n(\\{\\omega : \\omega_1 \\in \\{2, 3\\}\\})}{1440} = \\frac{n(\\{2, 3\\} \\times \\{1, \\dots, 10\\} \\times \\{1, \\dots, 24\\})}{1440} \\] \\[ = \\frac{2 \\times 10 \\times 24}{1440} = \\frac{480}{1440} = \\frac{1}{3} \\] (Este cálculo foi complicado e exigiu pensar nas outras features).\nCálculo no espaço induzido \\((\\mathbb{R}, \\mathcal{B}, \\mathbb{P}_X)\\): A v.a. \\(X\\) induz uma distribuição \\(\\mathbb{P}_X\\) que é simplesmente: \\[ \\mathbb{P}_X(B) = \\frac{n(B \\cap \\{1, 2, 3, 4, 5, 6\\})}{6}, \\quad B \\in \\mathcal{B}. \\] (Nós efetivamente “ignoramos” as features \\(F_2\\) e \\(F_3\\)). Para calcular \\(\\mathbb{P}(1.5 \\le X \\le 3.4)\\), nós perguntamos ao espaço induzido: \\[ \\mathbb{P}_X([1.5, 3.4]) = \\frac{n([1.5, 3.4] \\cap \\{1, 2, 3, 4, 5, 6\\})}{6} \\] \\[ = \\frac{n(\\{2, 3\\})}{6} = \\frac{2}{6} = \\frac{1}{3} \\] (Muito mais simples!)",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variáveis Aleatórias</span>"
    ]
  },
  {
    "objectID": "aula06.html#implementação-prática-em-r",
    "href": "aula06.html#implementação-prática-em-r",
    "title": "6  Variáveis Aleatórias",
    "section": "6.2 Implementação Prática em R",
    "text": "6.2 Implementação Prática em R\nVamos demonstrar como um data.frame é o nosso \\(\\Omega\\), e as colunas são as Variáveis Aleatórias.\n\n# Carregando pacotes\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\n\n# 1. Nosso Espaço Amostral Omega (Ω)\n# Cada linha (observação) é um resultado do experimento (ω)\n# O data.frame em si é o nosso universo Omega\nset.seed(42)\nOmega_data &lt;- tibble(\n  user_id = 1:1000,\n  # O 'evento bruto' ω pode ser um log complexo\n  raw_event_log = paste0(\"user_id: \", user_id, \", time: \", rnorm(1000, 50, 20)),\n  \n  # FEATURES (Variáveis Aleatórias)\n  # X é uma v.a. contínua (extraída do log)\n  X_session_duration = abs(rnorm(1000, 120, 40)), \n  \n  # Y é uma v.a. discreta (extraída do log)\n  Y_click_count = rpois(1000, 1.5),\n  \n  # Z é uma v.a. binária (Função Indicadora)\n  Z_is_vip = sample(c(0, 1), 1000, replace = TRUE, prob = c(0.9, 0.1))\n)\n\n\nhead(Omega_data)\n\n\n\n\n\n\nuser_id\nraw_event_log\nX_session_duration\nY_click_count\nZ_is_vip\n\n\n\n\n1\nuser_id: 1, time: 77.4191689429334\n213.00234\n2\n0\n\n\n2\nuser_id: 2, time: 38.7060365720782\n140.96489\n4\n0\n\n\n3\nuser_id: 3, time: 57.2625682267468\n158.82934\n1\n0\n\n\n4\nuser_id: 4, time: 62.6572520992208\n135.07894\n3\n0\n\n\n5\nuser_id: 5, time: 58.08536646282\n80.16266\n0\n0\n\n\n6\nuser_id: 6, time: 47.8775096781703\n96.10068\n1\n0\n\n\n\n\n\n\nV.A.s como Funções\nX_session_duration, Y_click_count e Z_is_vip são nossas v.a.s. Elas mapeiam o raw_event_log (ou user_id, nosso \\(\\omega\\)) para um número.\n\n\nExplorando Eventos \\(\\{X \\le x\\}\\)\nA “condição de mensurabilidade” \\(\\{X \\le x\\} \\in \\mathcal{F}\\) é o que nos permite usar o comando filter().\n\n# O evento A = {ω : X_session_duration(ω) &lt;= 30}\n# Este é o subconjunto de *usuários* (linhas) que satisfazem a condição.\nevento_A &lt;- Omega_data |&gt;\n  filter(X_session_duration &lt;= 30)\n\ncat(\"Número de usuários (ω) no evento {X &lt;= 30}:\", nrow(evento_A), \"\\n\")\n\nNúmero de usuários (ω) no evento {X &lt;= 30}: 13 \n\n# Podemos calcular a probabilidade deste evento\nP_A &lt;- nrow(evento_A) / nrow(Omega_data)\ncat(\"P(X &lt;= 30) =\", P_A, \"\\n\")\n\nP(X &lt;= 30) = 0.013 \n\n\n\n\nO Espaço Induzido \\(\\mathbb{P}_X\\) (O Histograma)\nNão precisamos mais do Omega_data (os logs brutos). Focamos apenas na distribuição da feature \\(X\\).\n\n# P_X é a \"distribuição de X\"\n# O histograma é a nossa visão empírica de P_X\n\nggplot(Omega_data, aes(x = X_session_duration)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = \"lightblue\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1, alpha= 0.9) +\n  labs(\n    title = \"Distribuição Induzida P_X (O Histograma)\",\n    subtitle = \"Jogamos Ω (os logs) fora e focamos no espaço induzido (os números em ℝ)\",\n    x = \"X_session_duration (Nossa V.A. Contínua)\",\n    y = \"Densidade (Probabilidade Induzida)\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.2.1 Distribuição Induzida de uma Função Indicadora \\(\\mathbb{P}_Z\\)\nPara a dummy variable \\(Z\\) (Z_is_vip), a distribuição \\(\\mathbb{P}_Z\\) é o que chamamos de Distribuição de Bernoulli. A “distribuição” de \\(Z\\) é apenas \\(P(Z=0)\\) e \\(P(Z=1)\\).\n\nOmega_data |&gt;\n  count(Z_is_vip) |&gt; # Agrupa pelos valores da v.a.\n  mutate(P_Z = n / sum(n)) # Calcula a probabilidade induzida\n\n\n\n\n\n\nZ_is_vip\nn\nP_Z\n\n\n\n\n0\n891\n0.891\n\n\n1\n109\n0.109",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Variáveis Aleatórias</span>"
    ]
  },
  {
    "objectID": "aula07.html",
    "href": "aula07.html",
    "title": "7  Função de Distribuição Acumulada (FDA)",
    "section": "",
    "text": "7.1 Implementação Prática em R\nNa aula anterior, definimos uma variável aleatória (v.a.) \\(X\\) como uma feature ou métrica (ex: session_duration).\nNa prática, especialmente com features contínuas, a pergunta “Qual a probabilidade da sessão durar exatamente 120.53 segundos?” não é útil (a resposta é zero!).\nAs perguntas de negócio que realmente importam são sobre limites (thresholds):\nA Função de Distribuição Acumulada (FDA), ou Cumulative Distribution Function (CDF), é a ferramenta matemática que responde a todas essas perguntas.\nComentário: Toda função F satisfazendo P1, P2, P3 é a função de distribuição de alguma variável aleatória.\nObservação: Uma função de distribuição pode corresponder a várias v.a.s no mesmo espaço de probabilidade. Por exemplo, se \\(X \\sim N(0, 1) \\rightarrow -X \\sim N(0, 1)\\) e \\(F_X = F_{-X}\\). No entanto \\(\\mathbb{P}(X = -X) = \\mathbb{P}(2X = 0) = \\mathbb{P}(X = 0) = 0\\).\nNotação: O símbolo “\\(\\sim\\)” significa “tem como distribuição” ou “está distribuído como”.\nNa prática, não conhecemos a \\(F_X\\) teórica; nós a estimamos a partir dos dados. Isso é chamado de Função de Distribuição Acumulada Empírica (eFDA).\nlibrary(tidyverse)\n\n# Usando os dados da aula anterior\nset.seed(42)\nOmega_data &lt;- tibble(\n  user_id = 1:1000,\n  X_session_duration = abs(rnorm(1000, 120, 40)), \n  Y_click_count = rpois(1000, 1.5),\n  Z_is_vip = sample(c(0, 1), 1000, replace = TRUE, prob = c(0.9, 0.1))\n)",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Função de Distribuição Acumulada (FDA)</span>"
    ]
  },
  {
    "objectID": "aula07.html#implementação-prática-em-r",
    "href": "aula07.html#implementação-prática-em-r",
    "title": "7  Função de Distribuição Acumulada (FDA)",
    "section": "",
    "text": "Calculando um Ponto na eFDA\nA definição \\(F_X(x) = \\mathbb{P}(X \\le x)\\) se traduz em R para: mean(X &lt;= x)\nIsso calcula a proporção de observações no dataset que são menores ou iguais a \\(x\\).\n\n# Qual a P(Duração da Sessão &lt;= 100 segundos)?\nx_threshold &lt;- 100\n\n# Usamos mean() que age como P(A) = count(A) / count(Omega)\nprob_acumulada &lt;- Omega_data  |&gt;\n  summarise(\n    F_X_de_100 = mean(X_session_duration &lt;= x_threshold)\n  )  |&gt;\n  pull(F_X_de_100)\n\ncat(\"Estimativa de F_X(100):\", prob_acumulada, \"\\n\")\n\nEstimativa de F_X(100): 0.307 \n\n\nIsso significa que 31% das sessões duram 100s ou menos.\n\n\nPlotando a eFDA Completa\nO ggplot2 tem uma função dedicada para plotar a eFDA: stat_ecdf(). Ela faz o cálculo acima para todos os valores de \\(x\\) e desenha a curva.\n\n# O valor que acabamos de calcular\nx_val &lt;- 100\ny_val &lt;- prob_acumulada\n\nggplot(Omega_data, aes(x = X_session_duration)) +\n  # Plota a eFDA\n  stat_ecdf(geom = \"step\", color = \"blue\", linewidth = 1) +\n  \n  # Linhas de anotação para o nosso ponto\n  geom_hline(yintercept = y_val, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = x_val, color = \"red\", linetype = \"dashed\") +\n  \n  # Anotação\n  annotate(\"text\", x = 150, y = y_val - 0.1, \n           label = paste0(\"P(X &lt;= 100) = \", round(y_val, 2)), \n           color = \"red\") +\n  \n  labs(\n    title = \"Função de Distribuição Acumulada Empírica (eFDA)\",\n    subtitle = \"Esta é a 'versão prática' da F_X(x) teórica\",\n    x = \"x (Duração da Sessão)\",\n    y = \"F_X(x) - Probabilidade Acumulada P(X &lt;= x)\"\n  )",
    "crumbs": [
      "Módulo Probabilidade",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Função de Distribuição Acumulada (FDA)</span>"
    ]
  },
  {
    "objectID": "020_inferencia.html",
    "href": "020_inferencia.html",
    "title": "Módulo Inferência Estatística",
    "section": "",
    "text": "TODO: descrever essa parte",
    "crumbs": [
      "Módulo Inferência Estatística"
    ]
  },
  {
    "objectID": "aula24.html",
    "href": "aula24.html",
    "title": "8  24 Introdução: O Dilema do Teste A/B",
    "section": "",
    "text": "8.1 O Framework da Inferência: Do Problema à Modelagem\nImagine que você é um Cientista de Dados em uma empresa de e-commerce. O time de design propõe um novo botão de “Comprar” (Versão B), com uma cor diferente, alegando que ele aumentará a taxa de cliques em relação ao botão atual (Versão A).\nPara validar essa hipótese, você implementa um teste A/B: 500 usuários aleatórios veem a Versão A, e outros 500 veem a Versão B. Ao final do experimento, você observa os resultados:\nA Versão B parece melhor. Mas a pergunta central que define a sua carreira como cientista é: essa diferença de 1% é real e significativa, ou pode ser apenas fruto do acaso? Se mostrarmos os botões para outros 500 usuários, talvez os resultados se invertam.\nPara responder a essa pergunta com confiança, precisamos de um framework rigoroso para “aprender” sobre a realidade a partir de dados limitados e ruidosos. Esta aula irá construir esse framework, peça por peça, usando a inferência estatística.\nO primeiro passo é traduzir nosso problema prático para uma linguagem matemática formal.",
    "crumbs": [
      "Módulo Inferência Estatística",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>24 Introdução: O Dilema do Teste A/B</span>"
    ]
  },
  {
    "objectID": "aula24.html#o-framework-da-inferência-do-problema-à-modelagem",
    "href": "aula24.html#o-framework-da-inferência-do-problema-à-modelagem",
    "title": "8  24 Introdução: O Dilema do Teste A/B",
    "section": "",
    "text": "Dados (observações): Os dados são valores observados de variáveis aleatórias que seguem uma distribuição de probabilidade conjunta P, que pertence a uma classe (conhecida) \\(\\mathcal{P}\\). Frequentemente, \\(\\mathcal{P}\\) é indexada por um parâmetro \\(\\theta \\in \\Theta\\). \\[ \\mathcal{P} = \\{ P_{\\theta}, \\theta \\in \\Theta \\} \\]\nObjetivo: fazer inferência sobre \\(\\theta\\) ou \\(g(\\theta)\\) com base nos dados observados.\n\nestimação pontual ou intervalar\nteste de hipóteses\n\n\n\nPerspectiva de Data Science:\n\nParâmetro (\\(\\theta\\)): A Verdade Oculta. \\(\\theta\\) é a verdadeira, mas desconhecida, taxa de cliques de um botão se pudéssemos mostrá-lo a um número infinito de usuários. É a realidade que queremos descobrir. No nosso caso, temos dois parâmetros de interesse: \\(\\theta_A\\) e \\(\\theta_B\\).\nModelo (\\(\\mathcal{P}\\)): Nossa Hipótese sobre o Mundo. \\(\\mathcal{P}\\) é a nossa escolha de modelagem. Ao rodar o teste A/B, assumimos que a decisão de cada usuário de clicar (ou não) é um evento independente, como um “cara ou coroa” com uma moeda viciada. Esse processo é descrito pela distribuição de Bernoulli. Portanto, nosso modelo para a Versão B é a família de todas as distribuições de Bernoulli, \\(\\mathcal{P} = \\{ \\text{Bernoulli}(\\theta_B), \\theta_B \\in [0, 1] \\}\\).",
    "crumbs": [
      "Módulo Inferência Estatística",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>24 Introdução: O Dilema do Teste A/B</span>"
    ]
  },
  {
    "objectID": "aula24.html#estimação-pontual-o-melhor-chute-a-partir-dos-dados",
    "href": "aula24.html#estimação-pontual-o-melhor-chute-a-partir-dos-dados",
    "title": "8  24 Introdução: O Dilema do Teste A/B",
    "section": "8.2 Estimação Pontual: O Melhor Chute a partir dos Dados",
    "text": "8.2 Estimação Pontual: O Melhor Chute a partir dos Dados\nNosso primeiro objetivo é usar os dados para dar um “chute” único e bem fundamentado sobre o valor do nosso parâmetro \\(\\theta\\).\nIngredientes:\n\nUma função real g, definida no espaço paramétrico \\(\\Theta\\), cujo valor \\(g(\\theta)\\) é o que gostaríamos de obter informação / estimar. \\(g(\\theta)\\): estimando.\nUm vetor aleatório \\(\\underline{X}\\) (observável) tomando valores no espaço amostral \\(\\mathcal{X}\\), de acordo com uma distribuição \\(P_{\\theta} \\in \\mathcal{P}\\). O valor observado de \\(\\underline{X}\\), \\(\\underline{x}\\) é o conjunto de dados. Muitas vezes, nos referimos a \\(\\underline{X} = (X_1, ..., X_n)\\) como amostra.\n\nIdeia: especificar um valor plausível para \\(g(\\theta)\\).\n\nDefinição 8.1 (Estatística e Estimador) Qualquer função da amostra \\(\\underline{X}\\) que não depende de quantidades desconhecidas é uma estatística. Uma estatística usada para estimar \\(g(\\theta)\\) é chamada de estimador.\nNotação:\n\nEstatística: \\(T = T(X_1, ..., X_n)\\)\nEstimador: \\(\\delta = \\delta(X_1, ..., X_n)\\) ou \\(\\hat{\\theta} = \\hat{\\theta}(X_1, ..., X_n)\\)\nValor observado do estimador, isto é \\(\\delta(\\underline{x})\\), é chamado de estimativa.\n\n\n\nPerspectiva de Data Science:\n\nAmostra (\\(\\underline{X}\\)): A Evidência Coletada. Para o botão B, nossa amostra é um vetor de 500 elementos, \\(\\underline{X} = (X_1, ..., X_{500})\\), onde \\(X_i=1\\) se o i-ésimo usuário clicou, e \\(X_i=0\\) caso contrário.\nEstimador (\\(\\hat{\\theta}\\)): Nosso Algoritmo de Aprendizagem. O estimador é a receita ou algoritmo que transforma os dados brutos em um chute para \\(\\theta\\). A receita mais intuitiva para estimar a taxa de cliques é simplesmente calcular a média da amostra: \\(\\hat{\\theta}_B = \\bar{X}_n = \\frac{1}{n}\\sum X_i\\).\nEstimativa: A estimativa é o número que nosso algoritmo produz: \\(\\hat{\\theta}_B(\\underline{x}) = 30/500 = 0.06\\).\n\n\n\nExemplo 8.1 (Tempo de Vida de Lâmpadas) Seja X = tempo de vida de lâmpadas de certa marca. Assuma que \\(X \\sim \\text{exp}(\\theta)\\), \\(\\theta &gt; 0\\). Suponha que \\((X_1, ..., X_n)\\) é uma a.a. de X.\nAqui, temos que \\(\\mathcal{P} = \\{f_{\\theta}, \\theta &gt; 0\\}\\), com \\(f_{\\theta}(x) = \\theta e^{-\\theta x} \\mathbb{I}_{(0, \\infty)}(x)\\).\nExemplos de estatísticas:\n\n\\(S_n = X_1 + ... + X_n\\) (tempo total de vida)\n\\(X_{(1)} = \\min\\{X_1, ..., X_n\\}\\) (menor tempo de vida)\n\\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) (média amostral dos tempos de vida)\n\\(S_n^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X}_n)^2\\) (variância amostral)\n\nQual é um estimador razoável para o tempo médio de vida? E para \\(\\theta\\)?\n\n\\(g(\\theta) = E_{\\theta}(X) = \\frac{1}{\\theta} \\rightarrow\\) possível estimador: \\(\\widehat{g(\\theta)} = \\bar{X}_n\\).\n\\(g(\\theta) = \\theta \\rightarrow\\) possível estimador: \\(\\hat{\\theta} = \\frac{1}{\\bar{X}_n}\\).",
    "crumbs": [
      "Módulo Inferência Estatística",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>24 Introdução: O Dilema do Teste A/B</span>"
    ]
  },
  {
    "objectID": "aula24.html#o-motor-da-inferência-a-função-de-verossimilhança",
    "href": "aula24.html#o-motor-da-inferência-a-função-de-verossimilhança",
    "title": "8  24 Introdução: O Dilema do Teste A/B",
    "section": "8.3 O Motor da Inferência: A Função de Verossimilhança",
    "text": "8.3 O Motor da Inferência: A Função de Verossimilhança\nTemos um algoritmo intuitivo para estimar \\(\\theta\\) (a média amostral), mas como podemos justificar que ele é um bom algoritmo? E se houvesse outros? A resposta está em um dos conceitos mais importantes da estatística e do Machine Learning: a verossimilhança.\nA verossimilhança responde à seguinte pergunta: “Dado os dados que observei, qual valor do parâmetro \\(\\theta\\) torna minhas observações mais prováveis (ou menos surpreendentes)?”\nEla funciona como uma função de pontuação (score) para diferentes hipóteses sobre a “verdade” \\(\\theta\\).\n\nDefinição 8.2 (Função de Verossimilhança) A função de verossimilhança de \\(\\theta \\in \\Theta\\), com base na amostra observada \\(\\underline{x} = (x_1, ..., x_n)\\), é dada por \\[ L(\\theta) = L(\\theta; \\underline{x}) = f_{X_1, ..., X_n}(x_1, ..., x_n; \\theta), \\quad \\theta \\in \\Theta \\]\nNota: se \\(\\underline{X}\\) é uma a.a. de X, então \\(L(\\theta) = \\prod_{i=1}^{n} f_X(x_i; \\theta)\\) (i.i.d.’s)\n\n\nExemplo 8.2 (Funções de Verossimilhança) Obtenha a função de verossimilhança em cada caso assumindo uma a.a. \\(\\underline{X} = (X_1, ..., X_n)\\) de X.\na) \\(X \\sim \\text{Bernoulli}(\\theta)\\) \\[ L(\\theta) = \\prod_{i=1}^{n} \\theta^{x_i} (1-\\theta)^{1-x_i} = \\theta^{\\sum_{i=1}^{n} x_i} (1-\\theta)^{n - \\sum_{i=1}^{n} x_i}, \\quad \\theta \\in (0,1). \\] \\(\\rightarrow L(\\theta)\\) depende da realização de \\(T = \\sum_{i=1}^{n} X_i\\).\n\nConexão com o Teste A/B: Esta é exatamente a função de verossimilhança para o nosso problema! Para o botão B, observamos \\(\\sum x_i = 30\\) e \\(n=500\\). A função se torna \\(L(\\theta_B) = \\theta_B^{30}(1-\\theta_B)^{470}\\). Agora podemos “testar” diferentes valores de \\(\\theta_B\\) e ver qual deles maximiza essa função. O valor que a maximiza é, de fato, \\(30/500 = 0.06\\), justificando nosso estimador intuitivo. Este é o Princípio da Máxima Verossimilhança.\n\nb) \\(X \\sim \\text{Poisson}(\\theta)\\) \\[ L(\\theta) = \\prod_{i=1}^{n} \\frac{e^{-\\theta} \\theta^{x_i}}{x_i!} = \\frac{e^{-n\\theta} \\theta^{\\sum_{i=1}^{n} x_i}}{\\prod_{i=1}^{n} x_i!}, \\quad \\theta &gt; 0. \\] \\(\\rightarrow L(\\theta)\\) depende da realização de \\(T = \\sum_{i=1}^{n} X_i\\).\nc) \\(X \\sim U(0, \\theta)\\), \\(\\theta &gt; 0\\) \\[ L(\\theta) = \\prod_{i=1}^{n} f_{\\theta}(x_i) = \\prod_{i=1}^{n} \\frac{1}{\\theta} \\mathbb{I}_{(0, \\theta)}(x_i) = \\frac{1}{\\theta^n} \\prod_{i=1}^{n} \\mathbb{I}_{(0, \\theta)}(x_i) \\] A indicadora \\(\\prod_{i=1}^{n} \\mathbb{I}_{(0, \\theta)}(x_i) = 1\\) se, e somente se, \\(0 &lt; x_i &lt; \\theta\\) para todo \\(i=1, ..., n\\), o que é equivalente a \\(0 &lt; x_{(1)} \\le ... \\le x_{(n)} &lt; \\theta\\). Então, \\[ L(\\theta) = \\frac{1}{\\theta^n} \\mathbb{I}_{(x_{(n)}, \\infty)}(\\theta), \\quad \\theta &gt; 0 \\] \\(\\rightarrow L(\\theta)\\) envolve a realização de \\(T = X_{(n)}\\).\nd) \\(X \\sim N(\\mu, \\sigma^2)\\), \\(\\theta = (\\mu, \\sigma^2)\\), \\(\\mu \\in \\mathbb{R}, \\sigma^2 &gt; 0\\) \\[ L(\\theta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i - \\mu)^2}{2\\sigma^2}} = \\frac{1}{(2\\pi\\sigma^2)^{n/2}} e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2} \\] \\[ = \\frac{1}{(2\\pi)^{n/2}} \\frac{1}{(\\sigma^2)^{n/2}} \\exp\\left\\{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i^2 - 2x_i\\mu + \\mu^2)\\right\\} \\] \\[ = \\frac{1}{(2\\pi)^{n/2}} \\frac{1}{(\\sigma^2)^{n/2}} \\exp\\left\\{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} x_i^2 + \\frac{\\mu}{\\sigma^2} \\sum_{i=1}^{n} x_i - \\frac{n\\mu^2}{2\\sigma^2}\\right\\}, \\quad \\mu \\in \\mathbb{R}, \\sigma^2 &gt; 0. \\] \\(\\rightarrow L(\\theta)\\) envolve a realização de \\(T_n = (\\sum_{i=1}^{n} X_i^2, \\sum_{i=1}^{n} X_i)\\).\n\n\nConexão com Aprendizagem Estatística: O processo de “treinar” um modelo de Aprendizagem Estatística (como uma Regressão Logística ou mesmo uma rede neural para classificação) é, em sua essência, um processo de otimização para encontrar os parâmetros do modelo (\\(\\theta\\)) que maximizam a função de verossimilhança (ou a log-verossimilhança) para os dados de treinamento. O framework que construímos aqui é a base teórica para a maior parte do aprendizado de máquina supervisionado.",
    "crumbs": [
      "Módulo Inferência Estatística",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>24 Introdução: O Dilema do Teste A/B</span>"
    ]
  },
  {
    "objectID": "aula24.html#implementação-prática-em-r",
    "href": "aula24.html#implementação-prática-em-r",
    "title": "8  24 Introdução: O Dilema do Teste A/B",
    "section": "8.4 Implementação Prática em R",
    "text": "8.4 Implementação Prática em R\n\nNa aula, afirmamos que a nossa estimativa intuitiva para a taxa de cliques (6%) era justificada pelo Princípio da Máxima Verossimilhança. Ou seja, de todas as “verdades” possíveis (\\(\\theta_B\\)), o valor \\(0.06\\) é o que torna os dados que realmente observamos (\\(k=30\\) cliques em \\(n=500\\) tentativas) os mais prováveis.\nVamos provar isso visualmente. Em vez de usar cálculo para encontrar o máximo da função, vamos simplesmente “testar” milhares de valores de \\(\\theta_B\\) e plotar a pontuação de verossimilhança que cada um recebe.\nConforme o Exemplo (a), a função de verossimilhança para um processo Bernoulli é:\n\\[L(\\theta) = \\theta^{k} (1-\\theta)^{n - k}\\]\nOnde:\n\n\\(n = 500\\) (visualizações da Versão B)\n\\(k = 30\\) (cliques na Versão B)\n\nNota Importante: \\(L(\\theta)\\) é um número absurdamente pequeno (ex: \\(0.06^{30} \\times (1-0.06)^{470}\\)). Computadores têm dificuldade com números tão próximos de zero. Por isso, na prática, nós sempre trabalhamos com a Log-Verossimilhança (ou “log-likelihood”).\n\\[\\ell(\\theta) = \\log(L(\\theta)) = k \\cdot \\log(\\theta) + (n - k) \\cdot \\log(1-\\theta)\\]\nEncontrar o \\(\\theta\\) que maximiza \\(L(\\theta)\\) é o mesmo que encontrar o \\(\\theta\\) que maximiza \\(\\ell(\\theta)\\), mas os números são muito mais estáveis.\nNão precisamos implementar essa função manualmente. O R já a possui: é a função de densidade da distribuição Binomial, dbinom(). Pedindo o logarítmico dela (log = TRUE), obtemos exatamente a log-verossimilhança.\nVamos: 1. Definir nossos dados observados. 2. Criar um “grid” de hipóteses para \\(\\theta_B\\) (ex: de 0.01 a 0.15). 3. Calcular a log-verossimilhança para cada hipótese. 4. Plotar e encontrar o pico.\n\nlibrary(ggplot2) # Para criar os gráficos\n\n# 1. Nossos dados observados para a Versão B\nn_B &lt;- 500\ncliques_B &lt;- 30\nestimativa_observada &lt;- cliques_B / n_B\n\n# 2. Criar um \"grid\" de hipóteses para a verdadeira taxa de cliques (theta_B)\n# Vamos testar 1000 valores possíveis entre 1% e 15%\nhipoteses_theta &lt;- seq(from = 0.01, to = 0.15, by = 0.0001)\n\n# 3. Calcular a log-verossimilhança para cada hipótese\n# Usamos dbinom() para calcular a \"pontuação\" de cada hipótese,\n# dado que observamos 'cliques_B' em 'n_B' tentativas.\n# log = TRUE nos dá a log-verossimilhança.\nlog_like &lt;- dbinom(x = cliques_B, \n                   size = n_B, \n                   prob = hipoteses_theta, \n                   log = TRUE)\n\n# 4. Preparar os dados para plotar\ndf_like &lt;- data.frame(\n  theta = hipoteses_theta,\n  log_likelihood = log_like\n)\n\n# 5. Encontrar o valor de theta que maximiza a log-verossimilhança\ntheta_max &lt;- hipoteses_theta[which.max(log_like)]\n\nprint(paste(\"Estimativa Observada (nosso 'chute'):\", estimativa_observada))\n\n[1] \"Estimativa Observada (nosso 'chute'): 0.06\"\n\nprint(paste(\"Estimativa de Máxima Verossimilhança (pico do gráfico):\", theta_max))\n\n[1] \"Estimativa de Máxima Verossimilhança (pico do gráfico): 0.06\"\n\n# 6. Plotar!\nggplot(df_like, aes(x = theta, y = log_likelihood)) +\n  geom_line(color = \"blue\", size = 1) +\n  # Adiciona uma linha vertical no pico encontrado\n  geom_vline(xintercept = theta_max, \n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  labs(\n    title = \"O Princípio da Máxima Verossimilhança na Prática\",\n    subtitle = paste(\"O pico da curva (vermelho) está em\", round(theta_max, 2), \"que é exatamente a nossa estimativa observada.\"),\n    x = \"Hipótese sobre a 'Verdadeira' Taxa de Cliques (theta_B)\",\n    y = \"Log-Verossimilhança (Pontuação da Hipótese)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nComo o gráfico demonstra, a função de log-verossimilhança atinge seu valor máximo exatamente em \\(\\theta = 0.06\\).\nIsso confirma nossa intuição: o “melhor chute” para a realidade desconhecida (\\(\\theta_B\\)) é, de fato, a média que observamos nos nossos dados. O que fizemos aqui foi validar nosso estimador intuitivo \\(\\hat{\\theta} = \\bar{X}_n\\) usando o rigoroso framework da Máxima Verossimilhança.",
    "crumbs": [
      "Módulo Inferência Estatística",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>24 Introdução: O Dilema do Teste A/B</span>"
    ]
  }
]